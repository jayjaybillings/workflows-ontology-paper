
@book{mcaffer_osgi_2010,
	address = {Boston},
	title = {{OSGi} and {Equinox}},
	publisher = {Addison-Wesley},
	author = {McAffer, Jeff and VanderLei, Paul and Archer, Simon},
	year = {2010}
}

@book{mcaffer_eclipse_2010,
	address = {Boston},
	title = {Eclipse {Rich} {Client} {Platform} {Second} {Edition}},
	publisher = {Addison-Wesley},
	author = {McAffer, Jeff and Lemieux, Jean-Michel and Aniszczyk, Chris},
	year = {2010}
}

@book{burke_restful_2010,
	address = {California},
	title = {{RESTful} {Java} with {JAX}-{RS}},
	publisher = {O-Reilly},
	author = {Burke, Bill},
	year = {2010}
}

@article{gaston_moose:_2009,
	title = {{MOOSE}: {A} parallel computational framework for coupled systems of nonlinear equations},
	volume = {239},
	number = {10},
	journal = {Nuclear Engineering and Design},
	author = {Gaston, Derek and Newman, Chris and Hansen, Glen and Lebrun-Grandie, Damien},
	month = oct,
	year = {2009}
}

@inproceedings{billings_designing_2009,
	address = {New York, NY, USA},
	series = {{CBHPC} '09},
	title = {Designing a {Component}-based {Architecture} for the {Modeling} and {Simulation} of {Nuclear} {Fuels} and {Reactors}},
	isbn = {978-1-60558-718-9},
	url = {http://doi.acm.org/10.1145/1687774.1687780},
	doi = {10.1145/1687774.1687780},
	abstract = {Concerns over the environment and energy security have recently prompted renewed interest in the U. S. in nuclear energy. Recognizing this, the U. S. Dept. of Energy has launched an initiative to revamp and modernize the role that modeling and simulation plays in the development and operation of nuclear facilities. This Nuclear Energy Advanced Modeling and Simulation (NEAMS) program represents a major investment in the development of new software, with one or more large multi-scale multi-physics capabilities in each of four technical areas associated with the nuclear fuel cycle, as well as additional supporting developments. In conjunction with this, we are designing a software architecture, computational environment, and component framework to integrate the NEAMS technical capabilities and make them more accessible to users. In this report of work very much in progress, we lay out the "problem" we are addressing, describe the model-driven system design approach we are using, and compare them with several large-scale technical software initiatives from the past. We discuss how component technology may be uniquely positioned to address the software integration challenges of the NEAMS program, outline the capabilities planned for the NEAMS computational environment and framework, and describe some initial prototyping activities.},
	urldate = {2016-05-25},
	booktitle = {Proceedings of the 2009 {Workshop} on {Component}-{Based} {High} {Performance} {Computing}},
	publisher = {ACM},
	author = {Billings, Jay J. and Elwasif, Wael R. and Hively, Lee M. and Bernholdt, David E. and Hetrick, III, John M. and Bohn, Tim},
	year = {2009},
	pages = {6:1--6:4},
	file = {ACM Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/QSFPPGE8/Billings et al. - 2009 - Designing a Component-based Architecture for the M.pdf:application/pdf}
}

@article{billings_domain-specific_2015,
	title = {A domain-specific analysis system for examining nuclear reactor simulation data for light-water and sodium-cooled fast reactors},
	volume = {85},
	issn = {0306-4549},
	url = {http://www.sciencedirect.com/science/article/pii/S0306454915003606},
	doi = {10.1016/j.anucene.2015.07.002},
	abstract = {Building a new generation of fission reactors in the United States presents many technical and regulatory challenges. One important challenge is the need to share and present results from new high-fidelity, high-performance simulations in an easily usable way. Since modern multiscale, multi-physics simulations can generate petabytes of data, they will require the development of new techniques and methods to reduce the data to familiar quantities of interest (e.g., pin powers, temperatures) with a more reasonable resolution and size. Furthermore, some of the results from these simulations may be new quantities for which visualization and analysis techniques are not immediately available in the community and need to be developed.

This paper describes a new system for managing high-performance simulation results in a domain-specific way that naturally exposes quantities of interest for light water and sodium-cooled fast reactors. It describes requirements to build such a system and the technical challenges faced in its development at all levels (simulation, user interface, etc.). An example comparing results from two different simulation suites for a single assembly in a light-water reactor is presented, along with a detailed discussion of the system’s requirements and design.},
	urldate = {2016-05-26},
	journal = {Annals of Nuclear Energy},
	author = {Billings, Jay Jay and Deyton, Jordan H. and Forest Hull III, S. and Lingerfelt, Eric J. and Wojtowicz, Anna},
	month = nov,
	year = {2015},
	keywords = {High-performance computing, LWR, Modeling, Nuclear reactors, SFR, simulation},
	pages = {856--868},
	file = {ScienceDirect Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/MNV6WXWF/Billings et al. - 2015 - A domain-specific analysis system for examining nu.pdf:application/pdf;ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/F4WWT46D/S0306454915003606.html:text/html}
}

@inproceedings{mccaskey_scientific_2015,
	address = {Reston VA},
	title = {Scientific {Simulation} with {Eclipse} - {From} {Zero} {Code} to {Running} on {Lots} of {Cores} in 10 {Minutes}},
	url = {https://www.eclipsecon.org/na2016/session/scientific-simulation-eclipse-zero-code-running-lots-cores-10-minutes},
	abstract = {Advanced modeling and simulation is revolutionizing the way we do science. It provides a means to test, predict, visualize, and analyze complex physical phenomena in a controlled manner that inevitably directs future theoretical and experimental endeavors. This truth is recognized by the scientific community, with much effort being directed at the development of advanced software frameworks to streamline the way we do computational science research.},
	urldate = {2016-05-26},
	booktitle = {{EclipseCon} {NA} 2016},
	publisher = {Eclipse Foundation},
	author = {McCaskey},
	month = nov,
	year = {2015},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/XSJZJK9N/scientific-simulation-eclipse-zero-code-running-lots-cores-10-minutes.html:text/html}
}

@article{humble_integrated_2014,
	title = {An integrated programming and development environment for adiabatic quantum optimization},
	volume = {7},
	issn = {1749-4699},
	url = {http://stacks.iop.org/1749-4699/7/i=1/a=015006},
	doi = {10.1088/1749-4680/7/1/015006},
	abstract = {Adiabatic quantum computing is a promising route to the computational power afforded by quantum information processing. The recent availability of adiabatic hardware has raised challenging questions about how to evaluate adiabatic quantum optimization (AQO) programs. Processor behavior depends on multiple steps to synthesize an adiabatic quantum program, which are each highly tunable. We present an integrated programming and development environment for AQO called Jade Adiabatic Development Environment (JADE) that provides control over all the steps taken during program synthesis. JADE captures the workflow needed to rigorously specify the AQO algorithm while allowing a variety of problem types, programming techniques, and processor configurations. We have also integrated JADE with a quantum simulation engine that enables program profiling using numerical calculation. The computational engine supports plug-ins for simulation methodologies tailored to various metrics and computing resources. We present the design, integration, and deployment of JADE and discuss its potential use for benchmarking AQO programs by the quantum computer science community.},
	language = {en},
	number = {1},
	urldate = {2016-05-25},
	journal = {Comput. Sci. Disc.},
	author = {Humble, T. S. and McCaskey, A. J. and Bennink, R. S. and Billings, J. J. and DʼAzevedo, E. F. and Sullivan, B. D. and Klymko, C. F. and Seddiqi, H.},
	year = {2014},
	pages = {015006},
	file = {IOP Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/UMPIV4KW/Humble et al. - 2014 - An integrated programming and development environm.pdf:application/pdf}
}

@inproceedings{billings_brand_2015,
	address = {Ludwigsburg Germany},
	title = {The brand new {Neutron} {Reflectivity} {Simulator} in {Eclipse} {ICE} and what it took to make it},
	url = {https://www.eclipsecon.org/europe2015/session/brand-new-neutron-reflectivity-simulator-eclipse-ice-and-what-it-took-make-it},
	abstract = {One common method of determining the exact structure of thin films is to put them into a beam of neutrons and see how the neutrons reflect off the surface. Such experiments require significant time and effort at one of a handful of facilities around the world since highly-collimated neutron beams are not generally available at local hardware stores. Simulating the profile of the reflected neutrons can thus greatly accelerate the research by providing insight on the best cases to study and keeping the final experiment focused and efficient.},
	urldate = {2016-05-25},
	booktitle = {{EclipseCon} {Europe} 2015},
	publisher = {Eclipse Foundation},
	author = {Billings, Jay Jay},
	month = jul,
	year = {2015},
	file = {billings_EclipseICE-NR.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/GKD9VKRF/billings_EclipseICE-NR.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/ZJSZWR2F/brand-new-neutron-reflectivity-simulator-eclipse-ice-and-what-it-took-make-it.html:text/html}
}

@article{pannala_multiscale_2015,
	title = {Multiscale modeling and characterization for performance and safety of lithium-ion batteries},
	volume = {118},
	issn = {0021-8979, 1089-7550},
	url = {http://scitation.aip.org/content/aip/journal/jap/118/7/10.1063/1.4927817},
	doi = {10.1063/1.4927817},
	abstract = {Lithium-ion batteries are highly complex electrochemical systems whose performance and safety are governed by coupled nonlinear electrochemical-electrical-thermal-mechanical processes over a range of spatiotemporal scales. Gaining an understanding of the role of these processes as well as development of predictive capabilities for design of better performing batteries requires synergy between theory, modeling, and simulation, and fundamental experimental work to support the models. This paper presents the overview of the work performed by the authors aligned with both experimental and computational efforts. In this paper, we describe a new, open source computational environment for battery simulations with an initial focus on lithium-ion systems but designed to support a variety of model types and formulations. This system has been used to create a three-dimensional cell and battery pack models that explicitly simulate all the battery components (current collectors, electrodes, and separator). The models are used to predict battery performance under normal operations and to study thermal and mechanical safety aspects under adverse conditions. This paper also provides an overview of the experimental techniques to obtain crucial validation data to benchmark the simulations at various scales for performance as well as abuse. We detail some initial validation using characterization experiments such as infrared and neutron imaging and micro-Raman mapping. In addition, we identify opportunities for future integration of theory, modeling, and experiments.},
	number = {7},
	urldate = {2016-05-26},
	journal = {Journal of Applied Physics},
	author = {Pannala, S. and Turner, J. A. and Allu, S. and Elwasif, W. R. and Kalnaus, S. and Simunovic, S. and Kumar, A. and Billings, J. J. and Wang, H. and Nanda, J.},
	month = aug,
	year = {2015},
	keywords = {Batteries, Cathodes, Electrolytes, Lithium, Thermal models},
	pages = {072017},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/39XC9ATU/Pannala et al. - 2015 - Multiscale modeling and characterization for perfo.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/C5PF6JCU/1.html:text/html}
}

@article{brooks_introducing_2016,
	series = {International {Conference} on {Computational} {Science} 2016, {ICCS} 2016, 6-8 {June} 2016, {San} {Diego}, {California}, {USA}},
	title = {Introducing {Triquetrum}, {A} {Possible} {Future} for {Kepler} and {Ptolemy} {II}},
	volume = {80},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050916310377},
	doi = {10.1016/j.procs.2016.05.546},
	abstract = {Triquetrum is an open platform for managing and executing scientific workflows that is under development as an Eclipse project. Both Triquetrum and Kepler use Ptolemy II as their execution engine. Triquetrum presents opportunities and risks for the Kepler community. The opportunities include a possibly larger community for interaction and a path for Kepler to move from Kepler's one-off ant-based build environment towards a more common OSGi-based environment and a way to maintain a stable Ptolemy II core. The risks include the fact that Triquetrum is a fork of Ptolemy II that would result in package name changes and other possible changes. In addition, Triquetrum is licensed under the Eclipse Public License v1.0, which includes a patent clause that could conflict with the University of California patent clause. This paper describes these opportunities and risks.},
	urldate = {2016-06-23},
	journal = {Procedia Computer Science},
	author = {Brooks, Christopher and Billings, Jay Jay},
	year = {2016},
	keywords = {Eclipse, Eclipse Public License, Kepler, Ptolemy II, Triquetrum, Visual Programming Systems, Workflow Management Systems},
	pages = {2449--2454},
	file = {ScienceDirect Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/HDJVZSWQ/Brooks and Billings - 2016 - Introducing Triquetrum, A Possible Future for Kepl.pdf:application/pdf;ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/FPRNXFFC/S1877050916310377.html:text/html}
}

@misc{billings_workflows_2016,
	title = {Workflows for {Rocket} {Scientists}/{Dummies}},
	url = {http://www.csm.ornl.gov/SOS20/agenda.html},
	urldate = {2016-08-28},
	author = {Billings, Jay Jay},
	month = mar,
	year = {2016},
	file = {billings_SOS20_20160323.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/B2GBHI2X/billings_SOS20_20160323.pdf:application/pdf;SOS20:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/EUD6H2MW/agenda.html:text/html}
}

@article{jha_building_2016,
	title = {A {Building} {Blocks} {Approach} towards {Domain} {Specific} {Workflow} {Systems}?},
	url = {http://arxiv.org/abs/1609.03484},
	abstract = {This paper makes the case for a fresh perspective on workflow-systems and, in doing so, argues for a building blocks approach to the design of workflow-systems. We outline a description of the building block approach and define their properties. We discuss RADICAL-Cybertools as an implementation of the building block concept, showing how they have been designed and developed in accordance to the building blocks method. Two use cases describe how {\textbackslash}rct building blocks have been used to develop and integrate scientific workflow systems illustrating the applicability and potential of software building blocks. In doing so we have begun an investigation of an alternative and conceptual approach to thinking the design and implementation of scientific workflow-systems.},
	urldate = {2016-12-14},
	journal = {arXiv:1609.03484 [cs]},
	author = {Jha, Shantenu and Turilli, Matteo},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03484},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv\:1609.03484 PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/KMW7UDH2/Jha and Turilli - 2016 - A Building Blocks Approach towards Domain Specific.pdf:application/pdf;arXiv.org Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/SJ77RI2V/1609.html:text/html}
}

@article{turilli_comprehensive_2015,
	title = {A {Comprehensive} {Perspective} on {Pilot}-{Job} {Systems}},
	url = {http://arxiv.org/abs/1508.04180},
	abstract = {Pilot-Job systems play an important role in supporting distributed scientific computing. They are used to consume more than 700 million CPU hours a year by the Open Science Grid communities, and by processing up to 1 million jobs a day for the ATLAS experiment on the Worldwide LHC Computing Grid. With the increasing importance of task-level parallelism in high-performance computing, Pilot-Job systems are also witnessing an adoption beyond traditional domains. Notwithstanding the growing impact on scientific research, there is no agreement upon a definition of Pilot-Job system and no clear understanding of the underlying abstraction and paradigm. Pilot-Job implementations have proliferated with no shared best practices or open interfaces and little interoperability. Ultimately, this is hindering the realization of the full impact of Pilot-Jobs by limiting their robustness, portability, and maintainability. This paper offers a comprehensive analysis of Pilot-Job systems critically assessing their motivations, evolution, properties, and implementation. The three main contributions of this paper are: (i) an analysis of the motivations and evolution of Pilot-Job systems; (ii) an outline of the Pilot abstraction, its distinguishing logical components and functionalities, its terminology, and its architecture pattern; and (iii) the description of core and auxiliary properties of Pilot-Jobs systems and the analysis of seven exemplar Pilot-Job implementations. Together, these contributions illustrate the Pilot paradigm, its generality, and how it helps to address some challenges in distributed scientific computing.},
	urldate = {2016-12-14},
	journal = {arXiv:1508.04180 [cs]},
	author = {Turilli, Matteo and Santcroos, Mark and Jha, Shantenu},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.04180},
	keywords = {68Nxx, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Software Engineering},
	file = {arXiv\:1508.04180 PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/K9JPBEIU/Turilli et al. - 2015 - A Comprehensive Perspective on Pilot-Job Systems.pdf:application/pdf;arXiv.org Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/AS6VMSIH/1508.html:text/html}
}

@article{yu_rutgers_2007,
	title = {The {Rutgers} {Workflow} {Management} {System}:  {Migrating} a {Digital} {Object} {Management} {Utility} to {Open} {Source}},
	issn = {1940-5758},
	shorttitle = {The {Rutgers} {Workflow} {Management} {System}},
	url = {http://journal.code4lib.org/articles/25},
	abstract = {This article examines the development, architecture, and future plans for the Workflow Management System, software developed by Rutgers University Libraries (RUL) to create and catalog digital objects for repository ingest and access. The Workflow Management System (WMS) was created as a front-end utility for the Fedora open source repository platform and a vehicle for a flexible, extensible metadata architecture, to serve the information needs of a large university and its collaborators. The next phase of development for the WMS shifted to a re-engineering of the WMS as an open source application. This paper discusses the design and architecture of the WMS, its re-engineering for open source release, remaining issues to be addressed before application release, and future development plans for the WMS.},
	number = {1},
	urldate = {2016-12-14},
	journal = {The Code4Lib Journal},
	author = {Yu, Grace Agnew \& Yang},
	month = dec,
	year = {2007},
	file = {Code4Lib Journal Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/X65DT7JT/25.html:text/html}
}

@article{rosemann_evaluation_1998,
	title = {Evaluation of {Workflow} {Management} {Systems} - {A} {Meta} {Model} {Approach}},
	volume = {6},
	copyright = {Copyright (c) 1969 Michael Rosemann, Michael zur Muehlen},
	issn = {1449-8618},
	url = {http://journal.acs.org.au/index.php/ajis/article/view/322},
	doi = {10.3127/ajis.v6i1.322},
	abstract = {The automated enactment of processes through the use of workflow management systems enables the outsourcing of the
control flow from application systems. By now a large number of systems, that follow different workflow paradigms, are
available. This leads to the problem of selecting the appropriate workflow management system for a given situation. In this paper we outline the benefits of a meta model approach for the evaluation and comparison of different workflow management systems. After a general introduction on the topic of meta modeling the meta models of the workflow management systems WorkParty (Siemens Nixdorf) and FlowMark (IBM) are compared as an example. These product specific meta models can be generalized to meta reference models, which helps to specify a workflow methodology. Exemplary, an organisational reference meta model is presented, which helps users in specifying their requirements for a workflow management system.},
	language = {en},
	number = {1},
	urldate = {2016-12-14},
	journal = {Australasian Journal of Information Systems},
	author = {Rosemann, Michael and Muehlen, Michael zur},
	year = {1998},
	keywords = {FlowMark, model, outsourcing, workflow management, WorkParty},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/UMV67PC8/Rosemann and Muehlen - 1998 - Evaluation of Workflow Management Systems - A Meta.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/EVB5ZDGV/322.html:text/html}
}

@book{allemang_semantic_2008,
	address = {Amsterdam},
	title = {Semantic web for the working ontologist: modeling in {RDF}, {RDFS} and {OWL}},
	isbn = {978-0-12-373556-0},
	shorttitle = {Semantic web for the working ontologist},
	language = {eng},
	publisher = {Morgan Kaufmann/Elsevier},
	author = {Allemang, Dean and Hendler, James A.},
	year = {2008},
	note = {OCLC: 184925396},
	keywords = {Metadata, Semantic Web, Web site development},
	file = {Ontologist.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/VVTDH84R/Ontologist.pdf:application/pdf}
}

@misc{vaadin_docs_2016,
	title = {Docs - vaadin.com {Creating} and {Running} a {Project} in {Eclipse} · {Vaadin}},
	url = {https://vaadin.com/docs},
	urldate = {2016-12-15},
	journal = {vaadin.com},
	author = {Vaadin},
	year = {2016},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/9UBCVJR9/getting-started-first-project.html:text/html}
}

@misc{vaadin_working_2016,
	title = {A working {Vaadin}, {Equinox}, {Jetty} application with {Eclipse} - {Wiki}},
	url = {https://vaadin.com/wiki},
	abstract = {For the last two weeks I\&\#39;ve been trying to develop a simple Vaadin application that could work as a stand alone OSGI bundle since Tomcat does not have support for OSGI. You could...},
	urldate = {2016-12-15},
	journal = {vaadin.com},
	author = {Vaadin},
	year = {2016},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/AC5SHRWM/wiki.html:text/html}
}

@misc{jekyll_build_2014,
	title = {Build {A} {Blog} {With} {Jekyll} {And} {GitHub} {Pages}},
	url = {https://www.smashingmagazine.com/2014/08/build-blog-jekyll-github-pages/},
	abstract = {Jekyll is a website generator that’s designed for building minimal, static blogs to be hosted on GitHub Pages. This article shows how to set it up.},
	urldate = {2016-12-19},
	journal = {Smashing Magazine},
	author = {Jekyll},
	month = aug,
	year = {2014},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/IWNAFWZ5/build-blog-jekyll-github-pages.html:text/html}
}

@inproceedings{barker_scientific_2007,
	title = {Scientific {Workflow}: {A} {Survey} and {Research} {Directions}},
	shorttitle = {Scientific {Workflow}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-68111-3_78},
	abstract = {Workflow technologies are emerging as the dominant approach to coordinate groups of distributed services. However with a space filled with competing specifications, standards and frameworks from multiple domains, choosing the right tool for the job is not always a straightforward task. Researchers are often unaware of the range of technology that already exists and focus on implementing yet another proprietary workflow system. As an antidote to this common problem, this paper presents a concise survey of existing workflow technology from the business and scientific domain and makes a number of key suggestions towards the future development of scientific workflow systems.},
	language = {en},
	urldate = {2016-12-20},
	booktitle = {Parallel {Processing} and {Applied} {Mathematics}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Barker, Adam and Hemert, Jano van},
	month = sep,
	year = {2007},
	note = {DOI: 10.1007/978-3-540-68111-3\_78},
	pages = {746--753},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/KS8U5A5X/Barker and Hemert - 2007 - Scientific Workflow A Survey and Research Directi.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/NHIJMFIX/978-3-540-68111-3_78.html:text/html}
}

@article{tolosana-calasanz_enforcing_2012,
	series = {{JCSS} {Special} {Issue}: {Cloud} {Computing} 2011},
	title = {Enforcing {QoS} in scientific workflow systems enacted over {Cloud} infrastructures},
	volume = {78},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000011001607},
	doi = {10.1016/j.jcss.2011.12.015},
	abstract = {The ability to support Quality of Service (QoS) constraints is an important requirement in some scientific applications. With the increasing use of Cloud computing infrastructures, where access to resources is shared, dynamic and provisioned on-demand, identifying how QoS constraints can be supported becomes an important challenge. However, access to dedicated resources is often not possible in existing Cloud deployments and limited QoS guarantees are provided by many commercial providers (often restricted to error rate and availability, rather than particular QoS metrics such as latency or access time). We propose a workflow system architecture which enforces QoS for the simultaneous execution of multiple scientific workflows over a shared infrastructure (such as a Cloud environment). Our approach involves multiple pipeline workflow instances, with each instance having its own QoS requirements. These workflows are composed of a number of stages, with each stage being mapped to one or more physical resources. A stage involves a combination of data access, computation and data transfer capability. A token bucket-based data throttling framework is embedded into the workflow system architecture. Each workflow instance stage regulates the amount of data that is injected into the shared resources, allowing for bursts of data to be injected while at the same time providing isolation of workflow streams. We demonstrate our approach by using the Montage workflow, and develop a Reference net model of the workflow.},
	number = {5},
	urldate = {2016-12-20},
	journal = {Journal of Computer and System Sciences},
	author = {Tolosana-Calasanz, Rafael and Bañares, José Ángel and Pham, Congduc and Rana, Omer F.},
	month = sep,
	year = {2012},
	keywords = {Cloud computing, Petri nets, QoS, scientific workflows},
	pages = {1300--1315},
	file = {ScienceDirect Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/HSW3FTJQ/Tolosana-Calasanz et al. - 2012 - Enforcing QoS in scientific workflow systems enact.pdf:application/pdf;ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/9WKRFSNV/S0022000011001607.html:text/html}
}

@article{chen_temporal_2011,
	title = {Temporal {Dependency}-based {Checkpoint} {Selection} for {Dynamic} {Verification} of {Temporal} {Constraints} in {Scientific} {Workflow} {Systems}},
	volume = {20},
	issn = {1049-331X},
	url = {http://doi.acm.org/10.1145/2000791.2000793},
	doi = {10.1145/2000791.2000793},
	abstract = {In a scientific workflow system, a checkpoint selection strategy is used to select checkpoints along scientific workflow execution for verifying temporal constraints so that we can identify any temporal violations and handle them in time in order to ensure overall temporal correctness of the execution that is often essential for the usefulness of execution results. The problem of existing representative strategies is that they do not differentiate temporal constraints as, once a checkpoint is selected, they verify all temporal constraints. However, such a checkpoint does not need to be taken for those constraints whose consistency can be deduced from others. The corresponding verification of such constraints is consequently unnecessary and can severely impact overall temporal verification efficiency while the efficiency determines whether temporal violations can be identified quickly for handling in time. To address the problem, in this article, we develop a new temporal-dependency based checkpoint selection strategy which can select checkpoints in accordance with different temporal constraints. With our strategy, the corresponding unnecessary verification can be avoided. The comparison and experimental simulation further demonstrate that our new strategy can improve the efficiency of overall temporal verification significantly over the existing representative strategies.},
	number = {3},
	urldate = {2016-12-20},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Chen, Jinjun and Yang, Yun},
	month = aug,
	year = {2011},
	keywords = {checkpoint selection, scientific workflows, temporal constraints, temporal verification},
	pages = {9:1--9:23},
	file = {ACM Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/HR7CUF2K/Chen and Yang - 2011 - Temporal Dependency-based Checkpoint Selection for.pdf:application/pdf}
}

@article{juve_scientific_2009,
	title = {Scientific {Workflow} {Applications} on {Amazon} {EC}2},
	url = {http://arxiv.org/abs/1005.2718},
	doi = {10.1109/ESCIW.2009.5408002},
	abstract = {The proliferation of commercial cloud computing providers has generated significant interest in the scientific computing community. Much recent research has attempted to determine the benefits and drawbacks of cloud computing for scientific applications. Although clouds have many attractive features, such as virtualization, on-demand provisioning, and "pay as you go" usage-based pricing, it is not clear whether they are able to deliver the performance required for scientific applications at a reasonable price. In this paper we examine the performance and cost of clouds from the perspective of scientific workflow applications. We use three characteristic workflows to compare the performance of a commercial cloud with that of a typical HPC system, and we analyze the various costs associated with running those workflows in the cloud. We find that the performance of clouds is not unreasonable given the hardware resources provided, and that performance comparable to HPC systems can be achieved given similar resources. We also find that the cost of running workflows on a commercial cloud can be reduced by storing data in the cloud rather than transferring it from outside.},
	urldate = {2016-12-20},
	journal = {arXiv:1005.2718 [astro-ph]},
	author = {Juve, Gideon and Deelman, Ewa and Vahi, Karan and Mehta, Gaurang and Berriman, Bruce and Berman, Benjamin P. and Maechling, Phil},
	month = dec,
	year = {2009},
	note = {arXiv: 1005.2718},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {59--66},
	file = {arXiv\:1005.2718 PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/VSQFV9XA/Juve et al. - 2009 - Scientific Workflow Applications on Amazon EC2.pdf:application/pdf;arXiv.org Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/M4GW9V87/1005.html:text/html}
}

@book{wainer_scientific_1997,
	title = {Scientific workflow systems},
	abstract = {This paper will discuss some of the basic assumptions behind office work and  scientific work, and show that workflow systems for these two endeavors should  have different functionalities. In particular we discuss the idea that data management  and management of workflow models are the most important aspects of  scientific workflows.},
	author = {Wainer, Jacques and Weske, Mathias and Vossen, Gottfried and Medeiros, Claudia Bauzer},
	year = {1997},
	file = {Citeseer - Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/UC6KVP2F/Wainer et al. - 1997 - Scientific workflow systems.pdf:application/pdf;Citeseer - Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/DZENGKJU/summary.html:text/html}
}

@incollection{weske_business_2012,
	title = {Business {Process} {Management} {Architectures}},
	copyright = {©2012 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-28615-5 978-3-642-28616-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-28616-2_7},
	abstract = {BPM architectures are in the centre of Chapter 7, starting from the WfMC Architecture and proceeding towards service oriented architectures and architectures for flexible workflow management. In particular, an architecture that allows to dynamically modify running workflow instances based on an object-oriented approach is introduced. Web services and their composition are sketched, describing the core concepts of the XML-based service composition language WS-BPEL. Advanced service composition based on semantic concepts are sketched, and case handling is introduced as a technique for flexible process enactment based on data dependencies rather than process structures.},
	language = {en},
	urldate = {2016-12-20},
	booktitle = {Business {Process} {Management}},
	publisher = {Springer Berlin Heidelberg},
	author = {Weske, Mathias},
	year = {2012},
	note = {DOI: 10.1007/978-3-642-28616-2\_7},
	keywords = {Computer Appl. in Administrative Data Processing, e-Commerce/e-business, Information Systems Applications (incl. Internet), IT in Business, Software Engineering},
	pages = {333--371},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/U69IB3PZ/Weske - 2012 - Business Process Management Architectures.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/XITST5VF/978-3-642-28616-2_7.html:text/html}
}

@article{chiu_meta_1999,
	series = {Meta-{Modelling} and {Methodology} {Engineering}},
	title = {A meta modeling approach to workflow management systems supporting exception handling},
	volume = {24},
	issn = {0306-4379},
	url = {http://www.sciencedirect.com/science/article/pii/S0306437999000101},
	doi = {10.1016/S0306-4379(99)00010-1},
	abstract = {Workflow Management Systems (WFMSs) facilitate the definition of structure and decomposition of business processes and assists in management of coordinating, scheduling, executing and monitoring of such activities. Most of the current WFMSs are built on traditional relational database systems and/or using an object-oriented database system for storing the definition and run time data about the workflows. However, a WFMS requires advanced modeling functionalities to support adaptive features, such as on-line exception handling. This article describes our advanced meta-modeling approach using various enabling technologies (such as object orientation, roles, rules, active capabilities) supported by an integrated environment, the ADOME, as a solid basis for a flexible WFMS involving dynamic match making, migrating workflows and exception handling.},
	number = {2},
	urldate = {2016-12-20},
	journal = {Information Systems},
	author = {Chiu, Dickson K. W. and Li, Qing and Karlapalem, Kamalakar},
	month = apr,
	year = {1999},
	keywords = {Exception Handling, Match-Making, Meta-modeling, Object-Orientation, Workflow Evolution, workflow management},
	pages = {159--184},
	file = {ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/WUQZNVQ5/S0306437999000101.html:text/html}
}

@book{aalst_workflow_2004,
	title = {Workflow {Management}: {Models}, {Methods}, and {Systems}},
	isbn = {978-0-262-72046-5},
	shorttitle = {Workflow {Management}},
	abstract = {This book offers a comprehensive introduction to workflow management, the management of business processes with information technology. By defining, analyzing, and redesigning an organization's resources and operations, workflow management systems ensure that the right information reaches the right person or computer application at the right time. The book provides a basic overview of workflow terminology and organization, as well as detailed coverage of workflow modeling with Petri nets. Because Petri nets make definitions easier to understand for nonexperts, they facilitate communication between designers and users. The book includes a chapter of case studies, review exercises, and a glossary. A special Web site developed by the authors, www.workflowcourse.com, features animation, interactive examples, lecture materials, exercises and solutions, relevant links, and other valuable resources for the classroom.},
	language = {en},
	publisher = {MIT Press},
	author = {Aalst, Wil van der and Hee, Kees Max van},
	year = {2004},
	keywords = {Business \& Economics / Management}
}

@book{taylor_workflows_2014,
	title = {Workflows for e-{Science}: {Scientific} {Workflows} for {Grids}},
	isbn = {978-1-84996-619-1},
	shorttitle = {Workflows for e-{Science}},
	abstract = {This is a timely book presenting an overview of the current state-of-the-art within established projects, presenting many different aspects of workflow from users to tool builders. It provides an overview of active research, from a number of different perspectives. It includes theoretical aspects of workflow and deals with workflow for e-Science as opposed to e-Commerce. The topics covered will be of interest to a wide range of practitioners.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
	year = {2014}
}

@inproceedings{mandal_integrating_2007,
	address = {New York, NY, USA},
	series = {{WORKS} '07},
	title = {Integrating {Existing} {Scientific} {Workflow} {Systems}: {The} {Kepler}/{Pegasus} {Example}},
	isbn = {978-1-59593-715-5},
	shorttitle = {Integrating {Existing} {Scientific} {Workflow} {Systems}},
	url = {http://doi.acm.org/10.1145/1273360.1273365},
	doi = {10.1145/1273360.1273365},
	abstract = {Scientific workflows have become an important tool used by scientists to conduct large-scale analysis in distributed environments. Today thereare a variety of workflow systems that provide an often disjoint set of capabilities and expose different workflow modeling semantics to the users. In this paper we examine the possibility of integrating two well-known workflow systems Kepler and Pegasus and examine the opportunities and challenges presented by such an integration. We illustrate the combined system on a workflow used as a basis of a provenance challenge.},
	urldate = {2016-12-20},
	booktitle = {Proceedings of the 2nd {Workshop} on {Workflows} in {Support} of {Large}-scale {Science}},
	publisher = {ACM},
	author = {Mandal, Nandita and Deelman, Ewa and Mehta, Gaurang and Su, Mei-Hui and Vahi, Karan},
	year = {2007},
	keywords = {programming models, scientific workflows, user interfaces},
	pages = {21--28},
	file = {ACM Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/NHBRSAGS/Mandal et al. - 2007 - Integrating Existing Scientific Workflow Systems .pdf:application/pdf}
}

@inproceedings{han_taxonomy_1998,
	title = {A taxonomy of adaptive workflow management},
	url = {https://www.researchgate.net/profile/Amit_Sheth/publication/228729962_A_taxonomy_of_adaptive_workflow_management/links/02bfe5105a211e3991000000.pdf},
	urldate = {2016-12-20},
	booktitle = {Workshop of the 1998 {ACM} {Conference} on {Computer} {Supported} {Cooperative} {Work}},
	author = {Han, Yanbo and Sheth, Amit and Bussler, Christoph},
	year = {1998},
	file = {A_taxonomy_of_adaptive_workflow_management.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/9A2AA6BJ/A_taxonomy_of_adaptive_workflow_management.pdf:application/pdf}
}

@article{davidson_provenance_2007,
	title = {Provenance in {Scientific} {Workflow} {Systems}.},
	volume = {30},
	url = {ftp://131.107.65.22/pub/debull/A07dec/susan.pdf},
	number = {4},
	urldate = {2016-12-20},
	journal = {IEEE Data Eng. Bull.},
	author = {Davidson, Susan B. and Boulakia, Sarah Cohen and Eyal, Anat and Ludäscher, Bertram and McPhillips, Timothy M. and Bowers, Shawn and Anand, Manish Kumar and Freire, Juliana},
	year = {2007},
	pages = {44--50},
	file = {C\:\\DEBull\\2007\\December\\SUSAN\\DataEngV4.dvi - susan.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/EVDTEXAR/susan.pdf:application/pdf}
}

@article{yu_taxonomy_2005,
	title = {A {Taxonomy} of {Workflow} {Management} {Systems} for {Grid} {Computing}},
	volume = {3},
	issn = {1570-7873, 1572-9184},
	url = {http://link.springer.com/article/10.1007/s10723-005-9010-8},
	doi = {10.1007/s10723-005-9010-8},
	abstract = {With the advent of Grid and application technologies, scientists and engineers are building more and more complex applications to manage and process large data sets, and execute scientific experiments on distributed resources. Such application scenarios require means for composing and executing complex workflows. Therefore, many efforts have been made towards the development of workflow management systems for Grid computing. In this paper, we propose a taxonomy that characterizes and classifies various approaches for building and executing workflows on Grids. We also survey several representative Grid workflow systems developed by various projects world-wide to demonstrate the comprehensiveness of the taxonomy. The taxonomy not only highlights the design and engineering similarities and differences of state-of-the-art in Grid workflow systems, but also identifies the areas that need further research.},
	language = {en},
	number = {3-4},
	urldate = {2016-12-20},
	journal = {J Grid Computing},
	author = {Yu, Jia and Buyya, Rajkumar},
	month = sep,
	year = {2005},
	pages = {171--200},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/HR9KQA46/Yu and Buyya - 2005 - A Taxonomy of Workflow Management Systems for Grid.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/77XI4GDC/s10723-005-9010-8.html:text/html}
}

@article{green_integrated_2000,
	series = {The 11th {International} {Conference} on {Advanced} {Information} {System} {Engineering}},
	title = {Integrated process modeling: {An} ontological evaluation},
	volume = {25},
	issn = {0306-4379},
	shorttitle = {Integrated process modeling},
	url = {http://www.sciencedirect.com/science/article/pii/S0306437900000107},
	doi = {10.1016/S0306-4379(00)00010-7},
	abstract = {Process modeling has gained prominence in the information systems modeling area due to its focus on business processes and its usefulness in such business improvement methodologies as Total Quality Management, Business Process Reengineering, and Workflow Management. However, process modeling techniques are not without their criticisms [13]. This paper proposes and uses the Bunge-Wand-Weber (BWW) representation model to analyze the five views — process, data, function, organization and output — provided in the Architecture of Integrated Information Systems (ARIS) popularized by Scheer [39, 40, 41]. The BWW representation model attempts to provide a theoretical base on which to evaluate and thus contribute to the improvement of information systems modeling techniques. The analysis conducted in this paper prompts some propositions. It confirms that the process view alone is not sufficient to model all the real-world constructs required. Some other symbols or views are needed to overcome these deficiencies. However, even when considering all five views in combination, problems may arise in representing all potentially required business rules, specifying the scope and boundaries of the system under consideration, and employing a “top-down” approach to analysis and design. Further work from this study will involve the operationalization of these propositions and their empirical testing in the field.},
	number = {2},
	urldate = {2016-12-20},
	journal = {Information Systems},
	author = {Green, Peter and Rosemann, Michael},
	month = apr,
	year = {2000},
	keywords = {Architecture of Integrated Information Systems, Ontology, Process Modeling},
	pages = {73--87},
	file = {ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/U59CCBWS/S0306437900000107.html:text/html}
}

@inproceedings{medina-mora_action_1992,
	address = {New York, NY, USA},
	series = {{CSCW} '92},
	title = {The {Action} {Workflow} {Approach} to {Workflow} {Management} {Technology}},
	isbn = {978-0-89791-542-7},
	url = {http://doi.acm.org/10.1145/143457.143530},
	doi = {10.1145/143457.143530},
	urldate = {2016-12-20},
	booktitle = {Proceedings of the 1992 {ACM} {Conference} on {Computer}-supported {Cooperative} {Work}},
	publisher = {ACM},
	author = {Medina-Mora, Raul and Winograd, Terry and Flores, Rodrigo and Flores, Fernando},
	year = {1992},
	keywords = {Action Workflow, coordination, coordinator, workflow},
	pages = {281--288}
}

@article{abramson_embedding_2010,
	title = {Embedding optimization in computational science workflows},
	volume = {1},
	issn = {1877-7503},
	url = {http://www.sciencedirect.com/science/article/pii/S1877750310000165},
	doi = {10.1016/j.jocs.2010.04.002},
	abstract = {Workflows support the automation of scientific processes, providing mechanisms that underpin modern computational science. They facilitate access to remote instruments, databases and parallel and distributed computers. Importantly, they allow software pipelines that perform multiple complex simulations (leveraging distributed platforms), with one simulation driving another. Such an environment is ideal for computational science experiments that require the evaluation of a range of different scenarios “in silico” in an attempt to find ones that optimize a particular outcome. However, in general, existing workflow tools do not incorporate optimization algorithms, and thus whilst users can specify simulation pipelines, they need to invoke the workflow as a stand-alone computation within an external optimization tool. Moreover, many existing workflow engines do not leverage parallel and distributed computers, making them unsuitable for executing computational science simulations. To solve this problem, we have developed a methodology for integrating optimization algorithms directly into workflows. We implement a range of generic actors for an existing workflow system called Kepler, and discuss how they can be combined in flexible ways to support various different design strategies. We illustrate the system by applying it to an existing bio-engineering design problem running on a Grid of distributed clusters.},
	number = {1},
	urldate = {2016-12-20},
	journal = {Journal of Computational Science},
	author = {Abramson, David and Bethwaite, Blair and Enticott, Colin and Garic, Slavisa and Peachey, Tom and Michailova, Anushka and Amirriazi, Saleh},
	month = may,
	year = {2010},
	keywords = {Cardiac models, Design optimization, Grid computing, Workflows},
	pages = {41--47},
	file = {ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/W7NR9F8V/S1877750310000165.html:text/html}
}

@article{ludascher_scientific_2006,
	title = {Scientific workflow management and the {Kepler} system},
	volume = {18},
	issn = {1532-0634},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.994/abstract},
	doi = {10.1002/cpe.994},
	abstract = {Many scientific disciplines are now data and information driven, and new scientific knowledge is often gained by scientists putting together data analysis and knowledge discovery ‘pipelines’. A related trend is that more and more scientific communities realize the benefits of sharing their data and computational services, and are thus contributing to a distributed data and computational community infrastructure (a.k.a. ‘the Grid’). However, this infrastructure is only a means to an end and ideally scientists should not be too concerned with its existence. The goal is for scientists to focus on development and use of what we call scientific workflows. These are networks of analytical steps that may involve, e.g., database access and querying steps, data analysis and mining steps, and many other steps including computationally intensive jobs on high-performance cluster computers. In this paper we describe characteristics of and requirements for scientific workflows as identified in a number of our application projects. We then elaborate on Kepler, a particular scientific workflow system, currently under development across a number of scientific data management projects. We describe some key features of Kepler and its underlying Ptolemy II system, planned extensions, and areas of future research. Kepler is a community-driven, open source project, and we always welcome related projects and new contributors to join. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {10},
	urldate = {2016-12-20},
	journal = {Concurrency Computat.: Pract. Exper.},
	author = {Ludäscher, Bertram and Altintas, Ilkay and Berkley, Chad and Higgins, Dan and Jaeger, Efrat and Jones, Matthew and Lee, Edward A. and Tao, Jing and Zhao, Yang},
	month = aug,
	year = {2006},
	keywords = {dataflow networks, Grid workflows, problem-solving environments, scientific data management, scientific workflows},
	pages = {1039--1065},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/WB4GUNBI/Ludäscher et al. - 2006 - Scientific workflow management and the Kepler syst.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/EJ3ZXKF5/abstract.html:text/html}
}

@article{yu_taxonomy_2005-1,
	title = {A {Taxonomy} of {Workflow} {Management} {Systems} for {Grid} {Computing}},
	url = {http://arxiv.org/abs/cs/0503025},
	abstract = {With the advent of Grid and application technologies, scientists and engineers are building more and more complex applications to manage and process large data sets, and execute scientific experiments on distributed resources. Such application scenarios require means for composing and executing complex workflows. Therefore, many efforts have been made towards the development of workflow management systems for Grid computing. In this paper, we propose a taxonomy that characterizes and classifies various approaches for building and executing workflows on Grids. We also survey several representative Grid workflow systems developed by various projects world-wide to demonstrate the comprehensiveness of the taxonomy. The taxonomy not only highlights the design and engineering similarities and differences of state-of-the-art in Grid workflow systems, but also identifies the areas that need further research.},
	urldate = {2016-12-20},
	journal = {arXiv:cs/0503025},
	author = {Yu, Jia and Buyya, Rajkumar},
	month = mar,
	year = {2005},
	note = {arXiv: cs/0503025},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, D.1},
	file = {arXiv\:cs/0503025 PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/FU4Z5SQW/Yu and Buyya - 2005 - A Taxonomy of Workflow Management Systems for Grid.pdf:application/pdf;arXiv.org Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/5MBD6NIE/0503025.html:text/html}
}

@article{liu_we_2014,
	title = {Do {We} {Need} to {Handle} {Every} {Temporal} {Violation} in {Scientific} {Workflow} {Systems}?},
	volume = {23},
	issn = {1049-331X},
	url = {http://doi.acm.org/10.1145/2559938},
	doi = {10.1145/2559938},
	abstract = {Scientific processes are usually time constrained with overall deadlines and local milestones. In scientific workflow systems, due to the dynamic nature of the underlying computing infrastructures such as grid and cloud, execution delays often take place and result in a large number of temporal violations. Since temporal violation handling is expensive in terms of both monetary costs and time overheads, an essential question aroused is “do we need to handle every temporal violation in scientific workflow systems?” The answer would be “true” according to existing works on workflow temporal management which adopt the philosophy similar to the handling of functional exceptions, that is, every temporal violation should be handled whenever it is detected. However, based on our observation, the phenomenon of self-recovery where execution delays can be automatically compensated for by the saved execution time of subsequent workflow activities has been entirely overlooked. Therefore, considering the nonfunctional nature of temporal violations, our answer is “not necessarily true.” To take advantage of self-recovery, this article proposes a novel adaptive temporal violation handling point selection strategy where this phenomenon is effectively utilised to avoid unnecessary temporal violation handling. Based on simulations of both real-world scientific workflows and randomly generated test cases, the experimental results demonstrate that our strategy can significantly reduce the cost on temporal violation handling by over 96\% while maintaining extreme low violation rate under normal circumstances.},
	number = {1},
	urldate = {2016-12-20},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Liu, Xiao and Yang, Yun and Yuan, Dong and Chen, Jinjun},
	month = feb,
	year = {2014},
	keywords = {quality of service, scientific workflows, temporal constraints, temporal verification, violation handling point selection},
	pages = {5:1--5:34},
	file = {ACM Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/9FQK3GJE/Liu et al. - 2014 - Do We Need to Handle Every Temporal Violation in S.pdf:application/pdf}
}

@inproceedings{altintas_provenance_2006,
	title = {Provenance {Collection} {Support} in the {Kepler} {Scientific} {Workflow} {System}},
	url = {http://link.springer.com/chapter/10.1007/11890850_14},
	abstract = {In many data-driven applications, analysis needs to be performed on scientific information obtained from several sources and generated by computations on distributed resources. Systematic analysis of this scientific information unleashes a growing need for automated data-driven applications that also can keep track of the provenance of the data and processes with little user interaction and overhead. Such data analysis can be facilitated by the recent advancements in scientific workflow systems. A major profit when using scientific workflow systems is the ability to make provenance collection a part of the workflow. Specifically, provenance should include not only the standard data lineage information but also information about the context in which the workflow was used, execution that processed the data, and the evolution of the workflow design. In this paper we describe a complete framework for data and process provenance in the Kepler Scientific Workflow System. We outline the requirements and issues related to data and workflow provenance in a multi-disciplinary workflow system and introduce how generic provenance capture can be facilitated in Kepler’s actor-oriented workflow environment. We also describe the usage of the stored provenance information for efficient rerun of scientific workflows.},
	language = {en},
	urldate = {2016-12-20},
	booktitle = {Provenance and {Annotation} of {Data}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Altintas, Ilkay and Barney, Oscar and Jaeger-Frank, Efrat},
	month = may,
	year = {2006},
	note = {DOI: 10.1007/11890850\_14},
	pages = {118--132},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/V5SRUKV7/Altintas et al. - 2006 - Provenance Collection Support in the Kepler Scient.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/HKMDRS99/11890850_14.html:text/html}
}

@article{liu_case_2005,
	title = {A case study of an inter-enterprise workflow-supported supply chain management system},
	volume = {42},
	issn = {0378-7206},
	url = {http://www.sciencedirect.com/science/article/pii/S037872060400045X},
	doi = {10.1016/j.im.2004.01.010},
	abstract = {Doing business over the Internet is cheap and convenient. This enlarges the view of enterprises and gives them an opportunity to select their partners. To support business-to-business operations, an information system (IS) with an embedded workflow management component is needed. The inherent characteristics of such a system makes it suitable to implement cross-organization management. Nowadays, however, these system additions are not common. When developing a supply chain management (SCM) system for a large motorcycle corporation in China, we had to construct an inter-enterprise architecture using the internet. The main part of this is a workflow-supported inner supply chain system and an integrated interface. In it, the business processes are defined and executed by the supply chain management system. The independent inner systems are connected by the integrated interface into a large, global, supply chain manage system to management business processes across the independent enterprises. This paper presents the system design and implementation and discusses our experiences and lessons learned.},
	number = {3},
	urldate = {2016-12-20},
	journal = {Information \& Management},
	author = {Liu, Jianxun and Zhang, Shensheng and Hu, Jinming},
	month = mar,
	year = {2005},
	keywords = {Agent, Electronic commerce, Interoperability, Supply chain, workflow management},
	pages = {441--454},
	file = {ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/PD5TJVH6/S037872060400045X.html:text/html}
}

@article{warr_scientific_2012,
	title = {Scientific workflow systems: {Pipeline} {Pilot} and {KNIME}},
	volume = {26},
	issn = {0920-654X, 1573-4951},
	shorttitle = {Scientific workflow systems},
	url = {http://link.springer.com/article/10.1007/s10822-012-9577-7},
	doi = {10.1007/s10822-012-9577-7},
	language = {en},
	number = {7},
	urldate = {2016-12-20},
	journal = {J Comput Aided Mol Des},
	author = {Warr, Wendy A.},
	month = jul,
	year = {2012},
	pages = {801--804},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/7VSNZZEI/Warr - 2012 - Scientific workflow systems Pipeline Pilot and KN.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/3S79SWGQ/10.html:text/html}
}

@inproceedings{wang_kepler_2009,
	address = {New York, NY, USA},
	series = {{WORKS} '09},
	title = {Kepler + {Hadoop}: {A} {General} {Architecture} {Facilitating} {Data}-intensive {Applications} in {Scientific} {Workflow} {Systems}},
	isbn = {978-1-60558-717-2},
	shorttitle = {Kepler + {Hadoop}},
	url = {http://doi.acm.org/10.1145/1645164.1645176},
	doi = {10.1145/1645164.1645176},
	abstract = {MapReduce provides a parallel and scalable programming model for data-intensive business and scientific applications. MapReduce and its de facto open source project, called Hadoop, support parallel processing on large datasets with capabilities including automatic data partitioning and distribution, load balancing, and fault tolerance management. Meanwhile, scientific workflow management systems, e.g., Kepler, Taverna, Triana, and Pegasus, have demonstrated their ability to help domain scientists solve scientific problems by synthesizing different data and computing resources. By integrating Hadoop with Kepler, we provide an easy-to-use architecture that facilitates users to compose and execute MapReduce applications in Kepler scientific workflows. Our implementation demonstrates that many characteristics of scientific workflow management systems, e.g., graphical user interface and component reuse and sharing, are very complementary to those of MapReduce. Using the presented Hadoop components in Kepler, scientists can easily utilize MapReduce in their domain-specific problems and connect them with other tasks in a workflow through the Kepler graphical user interface. We validate the feasibility of our approach via a word count use case.},
	urldate = {2016-12-20},
	booktitle = {Proceedings of the 4th {Workshop} on {Workflows} in {Support} of {Large}-{Scale} {Science}},
	publisher = {ACM},
	author = {Wang, Jianwu and Crawl, Daniel and Altintas, Ilkay},
	year = {2009},
	keywords = {distributed computing, easy-to-use, Hadoop, Kepler, MapReduce, parallel computing, scientific workflow},
	pages = {12:1--12:8},
	file = {ACM Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/U2HRFPQW/Wang et al. - 2009 - Kepler + Hadoop A General Architecture Facilitati.pdf:application/pdf}
}

@article{pizzi_aiida:_2016,
	title = {{AiiDA}: automated interactive infrastructure and database for computational science},
	volume = {111},
	issn = {0927-0256},
	shorttitle = {{AiiDA}},
	url = {http://www.sciencedirect.com/science/article/pii/S0927025615005820},
	doi = {10.1016/j.commatsci.2015.09.013},
	abstract = {Computational science has seen in the last decades a spectacular rise in the scope, breadth, and depth of its efforts. Notwithstanding this prevalence and impact, it is often still performed using the renaissance model of individual artisans gathered in a workshop, under the guidance of an established practitioner. Great benefits could follow instead from adopting concepts and tools coming from computer science to manage, preserve, and share these computational efforts. We illustrate here our paradigm sustaining such vision, based around the four pillars of Automation, Data, Environment, and Sharing. We then discuss its implementation in the open-source AiiDA platform (http://www.aiida.net), that has been tuned first to the demands of computational materials science. AiiDA’s design is based on directed acyclic graphs to track the provenance of data and calculations, and ensure preservation and searchability. Remote computational resources are managed transparently, and automation is coupled with data storage to ensure reproducibility. Last, complex sequences of calculations can be encoded into scientific workflows. We believe that AiiDA’s design and its sharing capabilities will encourage the creation of social ecosystems to disseminate codes, data, and scientific workflows.},
	urldate = {2016-12-23},
	journal = {Computational Materials Science},
	author = {Pizzi, Giovanni and Cepellotti, Andrea and Sabatini, Riccardo and Marzari, Nicola and Kozinsky, Boris},
	month = jan,
	year = {2016},
	keywords = {Directed acyclic graph, High-throughput, Materials database, Provenance, Reproducibility, scientific workflow},
	pages = {218--230},
	file = {ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/K4TGG25C/S0927025615005820.html:text/html}
}

@article{pizzi_aiida:_2016-1,
	title = {{AiiDA}: {Automated} interactive infrastructure and database for computational science},
	volume = {111},
	shorttitle = {{AiiDA}},
	url = {http://www.sciencedirect.com/science/article/pii/S0927025615005820},
	urldate = {2016-12-23},
	journal = {Computational Materials Science},
	author = {Pizzi, Giovanni and Cepellotti, Andrea and Sabatini, Riccardo and Marzari, Nicola and Kozinsky, Boris},
	year = {2016},
	pages = {218--230},
	file = {1504.01163v2.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/KINV6JNQ/1504.01163v2.pdf:application/pdf}
}

@techreport{deelman_future_2015,
	type = {Workshop},
	title = {The {Future} of {Scientific} {Workflows}},
	url = {http://science.energy.gov/~/media/ascr/pdf/programdocuments/docs/workflows_final_report.pdf},
	abstract = {Report of the DOE NGNS/CS Scientific Workflows Workshop},
	urldate = {2016-12-26},
	institution = {U.S. Department of Energy},
	author = {Deelman, Ewa and Peterka, Tom and Altintas, Ilkay and Carothers, Christopher and Kleese van Dam, Kerstin and Moreland, Kenneth and Ramakrishnan, Lavanya and Vetter, Jeffrey},
	month = apr,
	year = {2015},
	file = {workflows_report_v3.9 - workflows_final_report.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/QI5K4PF5/workflows_final_report.pdf:application/pdf}
}

@misc{jha_towards_2016,
	title = {Towards the {Science} of {Workflows}},
	author = {Jha, Shantenu},
	year = {2016},
	file = {workflow-design-study.v0.9.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/MXJ6W955/workflow-design-study.v0.9.pdf:application/pdf}
}

@misc{montoya_apex_2016,
	title = {{APEX} {Workflows}},
	url = {https://www.nersc.gov/assets/apex-workflows-v2.pdf},
	urldate = {2016-12-23},
	author = {Montoya, David},
	year = {2016},
	file = {apex-workflows-v2.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/JFBFHNBW/apex-workflows-v2.pdf:application/pdf}
}

@techreport{clay_incorporating_2015,
	title = {Incorporating {Workflow} for {V}\&v/{Uq} in the {Sandia} {Analysis} {Workbench}.},
	url = {https://www.osti.gov/scitech/biblio/1241123},
	language = {English},
	number = {SAND2015-1556C},
	urldate = {2016-12-26},
	institution = {Sandia National Laboratories (SNL-CA), Livermore, CA (United States)},
	author = {Clay, Robert L.},
	month = mar,
	year = {2015},
	file = {Clay - 2015 - Incorporating Workflow for V&vUq in the Sandia An.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/9NVG9RZM/Clay - 2015 - Incorporating Workflow for V&vUq in the Sandia An.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/HQ5IBTAF/1241123.html:text/html}
}

@misc{the_nek5000_team_nek5000_2014,
	title = {Nek5000 {\textbar} {A} {Spectral} {Element} code for {CFD}},
	url = {https://nek5000.mcs.anl.gov/},
	urldate = {2016-12-29},
	journal = {Nek5000 Website},
	author = {{The Nek5000 Team}},
	month = oct,
	year = {2014},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/92QJI68H/nek5000.mcs.anl.gov.html:text/html}
}

@article{abraham_gromacs:_2015,
	title = {{GROMACS}: {High} performance molecular simulations through multi-level parallelism from laptops to supercomputers},
	volume = {1–2},
	issn = {2352-7110},
	shorttitle = {{GROMACS}},
	url = {http://www.sciencedirect.com/science/article/pii/S2352711015000059},
	doi = {10.1016/j.softx.2015.06.001},
	abstract = {GROMACS is one of the most widely used open-source and free software codes in chemistry, used primarily for dynamical simulations of biomolecules. It provides a rich set of calculation types, preparation and analysis tools. Several advanced techniques for free-energy calculations are supported. In version 5, it reaches new performance heights, through several new and enhanced parallelization algorithms. These work on every level; SIMD registers inside cores, multithreading, heterogeneous CPU–GPU acceleration, state-of-the-art 3D domain decomposition, and ensemble-level parallelization through built-in replica exchange and the separate Copernicus framework. The latest best-in-class compressed trajectory storage format is supported.},
	urldate = {2016-12-30},
	journal = {SoftwareX},
	author = {Abraham, Mark James and Murtola, Teemu and Schulz, Roland and Páll, Szilárd and Smith, Jeremy C. and Hess, Berk and Lindahl, Erik},
	month = sep,
	year = {2015},
	keywords = {Free energy, GPU, Molecular dynamics, SIMD},
	pages = {19--25},
	file = {ScienceDirect Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/5EFZ79E7/Abraham et al. - 2015 - GROMACS High performance molecular simulations th.pdf:application/pdf;ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/9S45MWB3/S2352711015000059.html:text/html}
}

@misc{osgi_osgi_2016,
	title = {{OSGi}™ {Alliance} – {The} {Dynamic} {Module} {System} for {Java}},
	url = {https://www.osgi.org/},
	urldate = {2016-12-31},
	author = {OSGi},
	year = {2016},
	file = {OSGi™ Alliance – The Dynamic Module System for Java:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/ZT5IG9I7/www.osgi.org.html:text/html}
}

@misc{committers_equinox_2016,
	title = {Equinox},
	url = {http://www.eclipse.org/equinox/},
	abstract = {Eclipse is probably best known as a Java IDE, but it is more: it is an IDE framework, a tools framework, an open source project, a community, an eco-system, and a foundation.},
	urldate = {2016-12-31},
	author = {Committers, Equinox},
	year = {2016},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/MZIAJJH3/equinox.html:text/html}
}

@phdthesis{fielding_architectural_2000,
	title = {Architectural {Styles} and the {Design} of {Network}-based {Software} {Architectures}},
	school = {University of California, Irvine},
	author = {Fielding, Roy Thomas},
	year = {2000},
	note = {AAI9980887}
}

@misc{wikibooks_latex/tables_2016,
	title = {{LaTeX}/{Tables} - {Wikibooks}, open books for an open world},
	url = {https://en.wikibooks.org/wiki/LaTeX/Tables},
	urldate = {2017-01-01},
	author = {Wikibooks},
	year = {2016},
	file = {LaTeX/Tables - Wikibooks, open books for an open world:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/F92RCR6F/Tables.html:text/html}
}

@misc{pontesegger_eclipse_2015,
	title = {Eclipse {Advanced} {Scripting} {Environment}},
	url = {https://eclipse.org/ease/},
	abstract = {Eclipse is probably best known as a Java IDE, but it is more: it is an IDE framework, a tools framework, an open source project, a community, an eco-system, and a foundation.},
	urldate = {2017-01-01},
	author = {Pontesegger, Christian},
	year = {2015},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/8QFSHZW9/ease.html:text/html}
}

@misc{billings_eclipse_2016,
	type = {Project {Website}},
	title = {Eclipse {ICE}},
	shorttitle = {Eclipse {ICE}},
	url = {http://www.eclipse.org/ice/},
	language = {English},
	urldate = {2017-01-01},
	journal = {Eclipse ICE},
	author = {Billings, Jay Jay},
	month = oct,
	year = {2016},
	file = {Eclipse ICE:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/3WWF697A/ice.html:text/html}
}

@inproceedings{tibbitts_integrated_2009,
	address = {Los Alamitos, CA, USA},
	title = {An integrated approach to improving the parallel application development process},
	isbn = {978-1-4244-3751-1},
	doi = {10.1109/IPDPS.2009.5160941},
	abstract = {The development of parallel applications is becoming increasingly important to a broad range of industries. Traditionally, parallel programming was a niche area that was primarily exploited by scientists trying to model extremely complicated physical phenomenon. It is becoming increasingly clear, however, that continued hardware performance improvements through clock scaling and feature-size reduction are simply not going to be achievable for much longer. The hardware vendor's approach to addressing this issue is to employ parallelism through multi-processor and multi-core technologies. While there is little doubt that this approach produces scaling improvements, there are still many significant hurdles to be overcome before parallelism can be employed as a general replacement to more traditional programming techniques. The Parallel Tools Platform (PTP) Project was created in 2005 in an attempt to provide developers with new tools aimed at addressing some of the parallel development issues. Since then, the introduction of a new generation of peta-scale and multi-core systems has highlighted the need for such a platform. In this paper, we describe some of the challenges facing parallel application developers, present the current state of PTP, and provide a simple case study that demonstrates how PTP can be used to locate a potential deadlock situation in an MPI code.},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Parallel} \& {Distributed} {Processing} ({IPDPS})},
	publisher = {IEEE Computer Society},
	author = {Tibbitts, Beth R. and Watson, Gregory R. and Rasmussen, Craig E.},
	year = {2009},
	pages = {1--8},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/7VBSGBCP/05160941-abs.html:text/html}
}

@misc{billings_eclipse_2015,
	type = {Text},
	title = {Eclipse {Advanced} {Visualization} {Project}},
	url = {https://projects.eclipse.org/projects/science.eavp},
	abstract = {Visualization is a critical part of science and engineering projects and has roles in both setting up problems and post-processing results. The input or "construction" side can include things like constructing 3D geometries or volume meshes of physical space and the post-processing side can include everything from visualizing those geometries and meshes to plotting results to analyzing images to visualizing real data to almost everything else imagineable.},
	language = {und},
	urldate = {2017-01-01},
	journal = {projects.eclipse.org},
	author = {Billings, Jay Jay},
	month = dec,
	year = {2015},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/UX9AM9GR/science.html:text/html}
}

@misc{mccaskey_ornl-qci/xacc_2016,
	title = {{ORNL}-{QCI}/xacc},
	url = {https://github.com/ORNL-QCI/xacc},
	abstract = {xacc - XACC - eXtreme-scale Accelerator programming framework},
	urldate = {2017-01-01},
	journal = {GitHub},
	author = {McCaskey, Alex},
	year = {2016},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/Q4387NET/xacc.html:text/html}
}

@misc{brooks_triquetrum:_2015,
	title = {Triquetrum: {Models} of {Computation} for {Workflows}},
	shorttitle = {Triquetrum},
	url = {https://www.eclipsecon.org/na2016/session/triquetrum-models-computation-workflows},
	abstract = {Triquetrum is a new Eclipse project for managing and executing scientific workflows. The goal of Triquetrum is to support a wide range of use cases, ranging from automated processes based on predefined models, to replaying ad-hoc research workflows recorded from a user's actions in a scientific workbench UI. It will allow to define and execute models from personal pipelines with a few steps to massive models with thousands of elements.},
	urldate = {2017-01-01},
	journal = {EclipseCon NA 2016},
	author = {Brooks, Christopher},
	month = dec,
	year = {2015},
	file = {BrooksTriquetrumEclipseConNA8Mar2016.pptx - BrooksTriquetrumEclipseConNA08Mar2016.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/CH9WMBSM/BrooksTriquetrumEclipseConNA08Mar2016.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/6MZ9E6BX/triquetrum-models-computation-workflows.html:text/html}
}

@misc{graham_eclipse_2016,
	type = {Text},
	title = {Eclipse {January}},
	url = {https://projects.eclipse.org/projects/science.january},
	abstract = {Eclipse January is a set of libraries for handling numerical data in Java. It is inspired in part by NumPy and aims to provide similar functionality.},
	language = {und},
	urldate = {2017-01-01},
	journal = {projects.eclipse.org},
	author = {Graham, Jonah},
	month = apr,
	year = {2016},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/MUAQA4AJ/science.html:text/html}
}

@article{hollingsworth_workflow_1993,
	title = {Workflow management coalition the workflow reference model},
	volume = {68},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.198.5206&rep=rep1&type=pdf},
	urldate = {2017-01-03},
	journal = {Workflow Management Coalition},
	author = {Hollingsworth, David and Hampshire, U. K.},
	year = {1993},
	pages = {26},
	file = {RMV1-16.PDF - tc003v11.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/54VX8ZHD/tc003v11.pdf:application/pdf}
}

@misc{williams_6-year-old_2017,
	type = {Text.{Article}},
	title = {6-year-old accidentally orders high-end treats with {Amazon}'s {Alexa}},
	url = {http://www.foxnews.com/tech/2017/01/03/6-year-old-accidentally-orders-high-end-treats-with-amazons-alexa.html},
	abstract = {Amazon's Alexa sure is one high-class shopper.},
	language = {en-US},
	urldate = {2017-01-04},
	journal = {FoxNews.com},
	author = {Williams, Grace},
	month = jan,
	year = {2017},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/WPSSBQHR/6-year-old-accidentally-orders-high-end-treats-with-amazons-alexa.html:text/html}
}

@misc{_6-year-old_2016,
	title = {6-year-old girl used sleeping mom's fingerprints to buy {Pokemon} gifts},
	url = {http://www.dailymail.co.uk/~/article-4069090/index.html},
	abstract = {Ashlynd Howell, 6, of Maumelle, Arkansas, decided to do some shopping of her own on her mother's Amazon account, buying \$250 worth of Pokemon toys.},
	urldate = {2017-01-04},
	journal = {Mail Online},
	month = dec,
	year = {2016},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/8EPA93PQ/Mommy-shopping-Six-year-old-Arkansas-girl-used-sleeping-mom-s-fingerprints-unlock-iPhone-buy-13.html:text/html}
}

@misc{billings_[science-iwg]_2016,
	title = {[science-iwg] {Workflows}, workflows and more workflows},
	url = {https://dev.eclipse.org/mhonarc/lists/science-iwg/msg01699.html},
	urldate = {2017-01-04},
	author = {Billings, Jay Jay},
	month = apr,
	year = {2016},
	file = {[science-iwg] Workflows, workflows and more workflows:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/4WM329Q4/msg01699.html:text/html}
}

@book{weinberger_everything_2008,
	address = {New York, NY, USA},
	title = {Everything {Is} {Miscellaneous}: {The} {Power} of the {New} {Digital} {Disorder}},
	isbn = {978-0-8050-8811-3},
	shorttitle = {Everything {Is} {Miscellaneous}},
	abstract = {Human beings are information omnivores: we are constantly collecting, labeling, and organizing data. But today, the shift from the physical to the digital is mixing, burning, and ripping our lives apart. In the past, everything had its one place--the physical world demanded it--but now everything has its places: multiple categories, multiple shelves. Simply put, everything is suddenly miscellaneous. In Everything Is Miscellaneous, David Weinberger charts the new principles of digital order that are remaking business, education, politics, science, and culture. In his rollicking tour of the rise of the miscellaneous, he examines why the Dewey decimal system is stretched to the breaking point, how Rand McNally decides what information not to include in a physical map (and why Google Earth is winning that battle), how Staples stores emulate online shopping to increase sales, why your childrens teachers will stop having them memorize facts, and how the shift to digital music stands as the model for the future in virtually every industry. Finally, he shows how by "going miscellaneous," anyone can reap rewards from the deluge of information in modern work and life. From A to Z, Everything Is Miscellaneous will completely reshape the way you think--and what you know--about the world. The Flocking of Information: An Amazon.com Exclusive Essay by David Weinberger As businesses go miscellaneous, information gets chopped into smaller and smaller pieces. But it also escapes its leash--adding to a pile that can be sorted and arranged by anyone with a Web browser and a Net connection. In fact, information exhibits bird-like "flocking behavior," joining with other information that adds value to it, creating swarms that help customers and, ultimately, the businesses from which the information initially escaped. For example, Wize.com is a customer review site founded in 2005 by entrepreneur Doug Baker. The site provides reviews for everything from computers and MP3 players to coffee makers and baby strollers. But why do we need another place for reviews? If youre using the Web to research what digital camera to buy for your father-in-law, you probably feel there are far too many sites out there already. By the time you have scrolled through one stores customer reviews for each candidate camera and then cross-referenced the positive and the negative with the expert reviews at each of your bookmarked consumer magazines, you have to start the process again just to remember what people said. Wize in fact aims at exactly that problem. It pulls together reviews from many outside sources and aggregates them into three piles: user reviews, expert reviews (with links to the online publications), and the general "buzz." (For shoppers looking for a quick read on a product, Wize assigns an overall ranking.) When Wize reports that 97 percent of users love the Nikon D200 camera, it includes links to the online stores where the user reviews are posted, so customers are driven back to the businesses to spend their money. Zillow.com does something similar for real estate. The people behind Expedia.com, Rich Barton and Lloyd Frink, were looking for a new business idea--and were in the market for new homes. After hunting for information, they found that most of it was locked into the multiple listings sites of the National Association of Realtors. Now Zillow takes those listings and mashes them up with additional information that can help a potential purchaser find exactly what she wants. The most dramatic mashup right now is the "heat map" that uses swaths of color to let you tell at a glance what are the most expensive and most affordable areas. At some point, though, Zillow or one of its emerging competitors will mash up listing information with school ratings, crime maps, and aircraft flight patterns. Wize and Zillow make money by selling advertising, but their value is in the way their sites aggregate the miscellaneous--letting lots of independent sources flock together, all in one place. Were seeing the same trend in industry after industry, including music, travel, and the news media. Information gets released into the wild (sometimes against a companys will), where it joins up with other information, and the act of aggregating adds value. Companies lose some control, but they gain market presence and smarter customers. The companies that are succeeding in the new digital skies are the ones that allow their customers to add their own information and the aggregators to mix it up, because whether or not information wants to be free, it sure wants to flock.},
	publisher = {Henry Holt and Co., Inc.},
	author = {Weinberger, David},
	year = {2008}
}

@article{van_der_aalst_application_1998,
	title = {The application of petri nets to workflow management},
	volume = {08},
	issn = {0218-1266},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218126698000043},
	doi = {10.1142/S0218126698000043},
	abstract = {Workflow management promises a new solution to an age-old problem: controlling, monitoring, optimizing and supporting business processes. What is new about workflow management is the explicit representation of the business process logic which allows for computerized support. This paper discusses the use of Petri nets in the context of workflow management. Petri nets are an established tool for modeling and analyzing processes. On the one hand, Petri nets can be used as a design language for the specification of complex workflows. On the other hand, Petri net theory provides for powerful analysis techniques which can be used to verify the correctness of workflow procedures. This paper introduces workflow management as an application domain for Petri nets, presents state-of-the-art results with respect to the verification of workflows, and highlights some Petri-net-based workflow tools.},
	number = {01},
	urldate = {2017-01-05},
	journal = {J CIRCUIT SYST COMP},
	author = {Van Der Aalst, W. M. P.},
	month = feb,
	year = {1998},
	pages = {21--66},
	file = {Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/GFU9QSSP/S0218126698000043.html:text/html;Van Der Aalst - 1998 - The application of petri nets to workflow manageme.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/DHDWZMXN/Van Der Aalst - 1998 - The application of petri nets to workflow manageme.pdf:application/pdf}
}

@misc{panda_panda_2016,
	title = {{PanDA} {\textless} {PanDA} {\textless} {TWiki}},
	url = {https://twiki.cern.ch/twiki/bin/view/PanDA/PanDA},
	urldate = {2017-01-05},
	author = {PanDA},
	year = {2016},
	file = {PanDA < PanDA < TWiki:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/NWDPKZCD/PanDA.html:text/html}
}

@article{megino_panda:_2015,
	series = {4th {International} {Young} {Scientist} {Conference} on {Computational} {Science}},
	title = {{PanDA}: {Evolution} and {Recent} {Trends} in {LHC} {Computing}},
	volume = {66},
	issn = {1877-0509},
	shorttitle = {{PanDA}},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050915033992},
	doi = {10.1016/j.procs.2015.11.050},
	abstract = {The Large Hadron Collider(LHC) is the world's largest and most powerful machine. It started operating in 2009 with a scientific program foreseen to extend over the next coming decades at increasing energies and luminosities to maximise the discovery potential. During Run1 (2009- 2013), the Worldwide LHC Computing Grid (WLCG) successfully delivered all the necessary computing resources, which made the discovery of the Higgs Boson possible. Looking ahead, it is forecasted that increased luminosities will extrapolate to a multiplicity in the storage and processing costs, which is not reflected in a corresponding funding growth of the WLCG. ATLAS, one of the four experiments at the LHC, is therefore leading an upgrade program to evolve their software and computing model to make the best possible usage of available resources, and also leverage on upcoming state of the art computing paradigms that could make important resource contributions. These proceedings will give an insight into the accompanying work in PanDA, ATLAS’ workload management system. PanDA has implemented event level bookkeeping and dynamic generation of jobs with tailored lengths, in order to integrate and optimise the usage of oppor- tunistic resources, e.g. Cloud Computing or High Performance Computing (HPC). In conjunc- tion, the Event Service has been developed as a way to manage fine grained jobs and its outputs. Usage examples on some of the leading commercial and research infrastructures will be given. In addition, we will describe the work on further exploiting the current network capabilities by allowing remote data access and reducing regional boundaries.},
	urldate = {2017-01-05},
	journal = {Procedia Computer Science},
	author = {Megino, Fernando Barreiro and De, Kaushik and Caballero, Jose and Hover, John and Klimentov, Alexei and Maeno, Tadashi and Nilsson, Paul and Oleynik, Danila and Padolski, Sergey and Panitkin, Sergey and Petrosyan, Artem and Wenaus, Torre},
	month = jan,
	year = {2015},
	keywords = {ATLAS, Cloud computing, Grid computing, HPC, Workload Management System},
	pages = {439--447},
	file = {PanDA\: Evolution and Recent Trends in LHC Computing - 1-s2.0-S1877050915033992-main.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/6XCIZXSQ/1-s2.0-S1877050915033992-main.pdf:application/pdf;ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/QQMH9XE8/S1877050915033992.html:text/html}
}

@techreport{barker_high_2016,
	title = {High {Performance} {Computing} {Facility} {Operational} {Assessment} 2015: {Oak} {Ridge} {Leadership} {Computing} {Facility}},
	shorttitle = {High {Performance} {Computing} {Facility} {Operational} {Assessment} 2015},
	url = {https://www.osti.gov/scitech/biblio/1324094},
	language = {English},
	number = {ORNL/SPR--2016/110},
	urldate = {2017-01-05},
	institution = {Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States). Oak Ridge Leadership Computing Facility (OLCF)},
	author = {Barker, Ashley D. and Bernholdt, David E. and Bland, Arthur S. and Gary, Jeff D. and Hack, James J. and McNally, Stephen T. and Rogers, James H. and Smith, Brian E. and Straatsma, T. P. and Sukumar, Sreenivas Rangan and Thach, Kevin G. and Tichenor, Suzy and Vazhkudai, Sudharshan S. and Wells, Jack C.},
	month = mar,
	year = {2016},
	keywords = {mathematics and computing},
	file = {Barker et al. - 2016 - High Performance Computing Facility Operational As.pdf:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/R6A32ENF/Barker et al. - 2016 - High Performance Computing Facility Operational As.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/KE2568A9/1324094.html:text/html}
}

@misc{nersc_apex_2016,
	title = {{APEX}},
	url = {http://www.nersc.gov/research-and-development/apex/},
	urldate = {2017-01-05},
	author = {NERSC},
	year = {2016},
	file = {APEX:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/UUJKPTVP/apex.html:text/html}
}

@misc{pack_sos20_2016,
	title = {{SOS}20},
	url = {http://www.csm.ornl.gov/SOS20/agenda.html},
	urldate = {2017-01-05},
	author = {Pack, Daniel},
	month = mar,
	year = {2016},
	file = {SOS20:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/3ZRQPFJA/agenda.html:text/html}
}

@article{xsede_xsede_2013,
	title = {{XSEDE} {Cloud} {Survey} {Report}},
	url = {https://www.ideals.illinois.edu/handle/2142/45766},
	abstract = {A NSF-sponsored cloud user survey was conducted by XSEDE to better understand how cloud is used across a wide variety of scientific fields and the humanities, arts, and social sciences. Data was collected from 80 cloud users from around the globe. The project descriptions in this report illustrate the potential of cloud in accelerating research, enhancing collaboration, and enriching education. Cloud users provided extensive data on core usage, preferred storage, bandwidth, etc. and described cloud benefits and limitations for their specific use cases.},
	language = {en},
	urldate = {2017-01-05},
	author = {XSEDE},
	month = sep,
	year = {2013},
	file = {Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/4RRDA5KQ/2013 - XSEDE Cloud Survey Report.pdf:application/pdf;Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/SU3NUHAJ/45766.html:text/html}
}

@article{billings_eclipse_2017,
        title = {The {Eclipse} {Integrated} {Computational} {Environment}},
        url = {http://arxiv.org/abs/1704.01398},
        abstract = {Problems in modeling and simulation require significantly different workflow management technologies than standard grid-based workflow management systems. Computational scientists typically interact with simulation software in a feedback driven way were solutions and workflows are developed iteratively and simultaneously. This work describes common activities in workflows and how combinations of these activities form unique workflows. It presents the Eclipse Integrated Computational Environment as a workflow management system and development environment for the modeling and simulation community. Examples of the Environment's applicability to problems in energy science, general multiphysics simulations, quantum computing and other areas are presented as well as its impact on the community.},
        urldate = {2017-05-23},
        journal = {arXiv:1704.01398 [cs]},
        author = {Billings, Jay Jay and Bennett, Andrew R. and Deyton, Jordan and Gammeltoft, Kasper and Graham, Jonah and Gorin, Dasha and Krishnan, Hari and Li, Menghan and McCaskey, Alexander J. and Patterson, Taylor and Smith, Robert and Watson, Gregory R. and Wojtowicz, Anna},
        month = mar,
        year = {2017},
        note = {arXiv: 1704.01398},
        keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Software Engineering},
        file = {arXiv\:1704.01398 PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/TFFIW7U2/Billings et al. - 2017 - The Eclipse Integrated Computational Environment.pdf:application/pdf;arXiv.org Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/ZAXFHQRG/1704.html:text/html}
}

@article{lingerfelt_beam:_2016,
        series = {International {Conference} on {Computational} {Science} 2016, {ICCS} 2016, 6-8 {June} 2016, {San} {Diego}, {California}, {USA}},
        title = {{BEAM}: {A} {Computational} {Workflow} {System} for {Managing} and {Modeling} {Material} {Characterization} {Data} in {HPC} {Environments}},
        volume = {80},
        issn = {1877-0509},
        shorttitle = {{BEAM}},
        url = {http://www.sciencedirect.com/science/article/pii/S1877050916308869},
        doi = {10.1016/j.procs.2016.05.410},
        abstract = {Improvements in scientific instrumentation allow imaging at mesoscopic to atomic length scales, many spectroscopic modes, and now—with the rise of multimodal acquisition systems and the associated processing capability—the era of multidimensional, informationally dense data sets has arrived. Technical issues in these combinatorial scientific fields are exacerbated by computational challenges best summarized as a necessity for drastic improvement in the capability to transfer, store, and analyze large volumes of data. The Bellerophon Environment for Analysis of Materials (BEAM) platform provides material scientists the capability to directly leverage the integrated computational and analytical power of High Performance Computing (HPC) to perform scalable data analysis and simulation via an intuitive, cross-platform client user interface. This framework delivers authenticated, “push-button” execution of complex user workflows that deploy data analysis algorithms and computational simulations utilizing the converged compute-and-data infrastructure at Oak Ridge National Laboratory's (ORNL) Compute and Data Environment for Science (CADES) and HPC environments like Titan at the Oak Ridge Leadership Computing Facility (OLCF). In this work we address the underlying HPC needs for characterization in the material science community, elaborate how BEAM's design and infrastructure tackle those needs, and present a small sub-set of user cases where scientists utilized BEAM across a broad range of analytical techniques and analysis modes.},
        urldate = {2017-05-23},
        journal = {Procedia Computer Science},
        author = {Lingerfelt, E. J. and Belianinov, A. and Endeve, E. and Ovchinnikov, O. and Somnath, S. and Borreguero, J. M. and Grodowitz, N. and Park, B. and Archibald, R. K. and Symons, C. T. and Kalinin, S. V. and Messer, O. E. B. and Shankar, M. and Jesse, S.},
        month = jan,
        year = {2016},
        keywords = {Computational workflows, data management, HPC workflows, materials modeling, materials science, multi-tier architectures, scalable data analysis, user experience design},
        pages = {2276--2280},
        file = {ScienceDirect Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/NJ3IGH5A/Lingerfelt et al. - 2016 - BEAM A Computational Workflow System for Managing.pdf:application/pdf;ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/I785X9PK/S1877050916308869.html:text/html}
}

@article{deelman_future_2017,
        title = {The future of scientific workflows},
        issn = {1094-3420},
        url = {http://dx.doi.org/10.1177/1094342017704893},
        doi = {10.1177/1094342017704893},
        abstract = {Today’s computational, experimental, and observational sciences rely on computations that involve many related tasks. The success of a scientific mission often hinges on the computer automation of these workflows. In April 2015, the US Department of Energy (DOE) invited a diverse group of domain and computer scientists from national laboratories supported by the Office of Science, the National Nuclear Security Administration, from industry, and from academia to review the workflow requirements of DOE’s science and national security missions, to assess the current state of the art in science workflows, to understand the impact of emerging extreme-scale computing systems on those workflows, and to develop requirements for automated workflow management in future and existing environments. This article is a summary of the opinions of over 50 leading researchers attending this workshop. We highlight use cases, computing systems, workflow needs and conclude by summarizing the remaining challenges this community sees that inhibit large-scale scientific workflows from becoming a mainstream tool for extreme-scale science.},
        language = {en},
        urldate = {2017-05-10},
        journal = {The International Journal of High Performance Computing Applications},
        author = {Deelman, Ewa and Peterka, Tom and Altintas, Ilkay and Carothers, Christopher D and Kleese van Dam, Kerstin and Moreland, Kenneth and Parashar, Manish and Ramakrishnan, Lavanya and Taufer, Michela and Vetter, Jeffrey},
        month = apr,
        year = {2017},
        pages = {1094342017704893},
        file = {SAGE PDF Full Text:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/U8BJFSRM/Deelman et al. - 2017 - The future of scientific workflows.pdf:application/pdf}
}

@article{ferreira_da_silva_characterization_nodate,
        title = {A characterization of workflow management systems for extreme-scale applications},
        issn = {0167-739X},
        url = {http://www.sciencedirect.com/science/article/pii/S0167739X17302510},
        doi = {10.1016/j.future.2017.02.026},
        abstract = {Automation of the execution of computational tasks is at the heart of improving scientific productivity. Over the last years, scientific workflows have been established as an important abstraction that captures data processing and computation of large and complex scientific applications. By allowing scientists to model and express entire data processing steps and their dependencies, workflow management systems relieve scientists from the details of an application and manage its execution on a computational infrastructure. As the resource requirements of today’s computational and data science applications that process vast amounts of data keep increasing, there is a compelling case for a new generation of advances in high-performance computing, commonly termed as extreme-scale computing, which will bring forth multiple challenges for the design of workflow applications and management systems. This paper presents a novel characterization of workflow management systems using features commonly associated with extreme-scale computing applications. We classify 15 popular workflow management systems in terms of workflow execution models, heterogeneous computing environments, and data access methods. The paper also surveys workflow applications and identifies gaps for future research on the road to extreme-scale workflows and management systems.},
        urldate = {2017-02-21},
        journal = {Future Generation Computer Systems},
        author = {Ferreira da Silva, Rafael and Filgueira, Rosa and Pietri, Ilia and Jiang, Ming and Sakellariou, Rizos and Deelman, Ewa},
        keywords = {Extreme-scale computing, in situ processing, scientific workflows, Workflow Management Systems},
        file = {ScienceDirect Full Text PDF:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/TE4UC2KD/Ferreira da Silva et al. - A characterization of workflow management systems .pdf:application/pdf;ScienceDirect Snapshot:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/K774SQ5T/S0167739X17302510.html:text/html}
}

@misc{noauthor_cesm_nodate,
	title = {{CESM} {Workflow} - {CSEG} - {CESM} {Model} {Work} {Flow} - wiki.ucar.edu},
	url = {https://wiki.ucar.edu/display/CESMworkFlow/CESM+Workflow},
	urldate = {2017-01-10},
	author = {Mai, Andrew},
	month = nov,
	year = {2013},
	file = {CESM Workflow - CSEG - CESM Model Work Flow - wiki.ucar.edu:/home/bkj/.zotero/zotero/xvg75ijy.default/zotero/storage/7I834PDH/CESM+Workflow.html:text/html}
}


@article{atkinson-csur,
 author = {Liew, Chee Sun and Atkinson, Malcolm P. and Galea, Michelle and Ang, Tan Fong and Martin, Paul and Hemert, Jano I. Van},
 title = {Scientific Workflows: Moving Across Paradigms},
 journal = {ACM Comput. Surv.},
 issue_date = {February 2017},
 volume = {49},
 number = {4},
 month = dec,
 year = {2016},
 issn = {0360-0300},
 pages = {66:1--66:39},
 articleno = {66},
 numpages = {39},
 url = {http://doi.acm.org/10.1145/3012429},
 doi = {10.1145/3012429},
 acmid = {3012429},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data-intensive science, workflow management systems, workflows},
} 
