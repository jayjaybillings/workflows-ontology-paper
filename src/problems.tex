\chapter{Problems}

The previous review of workflows and workflow management systems as well as the
discussion of Eclipse ICE illustrated the diverse nature of scientific
workflows. There are many problems that arise from this diversity the community
would stand to benefit if they could be addressed. However, it should be clear
from the previous sections that there are two significant problems facing the workflow science community:
\begin{itemize}
  \item Everyone has their own workflow tools, and
no two tool chains are the same.
  \item There is very little interoperability in the workflow tool space.
\end{itemize}

The following sections elaborate on community drivers for providing
interoperability between workflow management systems.

\section{A Buildings Block Approach}\label{a-buildings-block-approach}

For many reasons, the traditional approach for building workflow systems
has been to build as much of the required capability as possible into
the system itself, relying very little on external services or even
third party code. This does not track will with the course of history
since high-level functionality tends to slowly creep down the
software stack and into kernels, kernel services, and system libraries. Jha and
Turilli discuss this trend as it relates to workflows from a cyber
infrastructure perspective and to existing large scale scientific
workflow efforts, \cite{jha_building_2016}. They propose that, while historically
successful, monolithic workflow systems present many problems for users,
developers, and maintainers. Instead, they propose that a new ``Lego
style'' approach might work better where individual ``building blocks''
of capability are assembled into the final workflow product. These
building blocks would include things like programming interfaces to
queuing systems, programmable pilot systems for scheduling jobs,
workload balancers, and ensemble execution tools, among others.

This approach would greatly improve both interoperability and sustainability
because it would standardize the programming interfaces and backends used by
workflow management systems. Jha et al. are developing a white paper to address
this further, \cite{jha_towards_2016}.

\section{SOS20 Discussion on
Workflows}\label{sos20-discussion-on-workflows}

Session IV of the Twentieth Anniversary Meeting of the SOS Workshop
(SOS20) focused on workflow and workflow management
system development activities of the three participating institutions: Sandia
National Laboratory, Oak Ridge National Laboratory, and the Swiss National Supercomputing Centre, \cite{pack_sos20_2016}. Multiple
presenters illustrated the challenges facing the workflow science community and
organized a separate, spontaneous ``Birds of a Feather'' (BOF) session at
the end of the event. The BOF attendees widely agreed after a thorough
discussion that no one single workflow management system could satisfy
all the needs of those present, but that the community as a whole would
be served best by seeking to enable interoperability where possible. This
discussion led to a wider engagement of the community by BOF attendees
who decided that it would be best to engage in an open public forum so that
other groups could join the effort, \cite{billings_[science-iwg]_2016}.

\section{High-Performance Computing
Drivers}\label{high-performance-computing-drivers}

The large number of workflow management systems and lack of
interoperability is a significant barrier to their adoption in
high-performance computing facilities such as the U.S. Department of Energy's
Leadership Computing Facilities. The high performance computing Facility
Operational Assessment 2015: Oak Ridge Leadership Computing Facility
report, \cite{barker_scientific_2007}, illustrates the problem that such facilities
face,

\begin{displayquote}
These discussions concluded with the observation that the current proliferation
of workflow systems in response to perceived domain-specific needs of 
scientific workflows makes it difficult to choose a site-wide operational
workflow manager, particularly for the leadership-class machines. However,
there are opportunities where facilities can centralize workflow technology 
offerings to reduce anticipated fragmentation. This is especially true if a 
facility attempts to develop, deploy, and operate each and every workflow 
solution requested by the user community. Through these evaluations, the OLCF
seeks to identify interesting intersections that are of the most value to OLCF
stakeholders.
\end{displayquote}

The Oak Ridge Leadership Computing Facility (OLCF) is not alone in this
as other Leadership Computing Facilities face the same problem. Few
of these facilities have discussed the possibility they may end up
supporting different workflows systems entirely such that workflows at
one facility can not be run at another without significant work to
install an additional workflow management system!

Developing interoperability layers based on common building blocks with
standard service interfaces would address both of these problems by
allowing facilities to run various workflows on the system that worked
best for their hardware and software configuration.

\section{Energy Science Drivers}\label{energy-science-drivers}

The Future of Scientific Workflows report by Deelman et al. illustrates
the idea of the ``large-scale science campaign,'' \cite{deelman_future_2015}.
Such a campaign integrates multiple workflows to perform data
acquisition from experimental equipment, modeling and analysis with
supercomputers, and data analysis with either grids computing or
supercomputers. This is, in essence, the same problem that
PanDA/BigPanDA solves for the Large Hadron Collider. So, while it is not
a new problem, it is increasingly common. One solution to this problem
is to support all the possible workflow management systems, but that
becomes quickly unsustainable, as the OLCF learned through experience.
Enabling interoperability is a much more sustainable solution.

An interesting case study is the ICEMAN project at Oak Ridge National
Laboratory, on which the author is a co-principal investigator. ICEMAN
seeks to use a combination of instruments from the Spallation Neutron
Source, OLCF compute resources such as the Titan supercomputer, the
Laboratory's Compute Advanced Data Environment for Science (CADES),
Eclipse ICE, and AiiDA (c.f. - \cite{pizzi_aiida:_2016}) to automate the
analysis of quasielastic neutron scattering data and provide a
comprehensive problem solving environment for scientists.
