\section{The Solution: Common Building Blocks}\label{buildings-blocks}

The rich landscape of workflow tools suggests that the  challenge is not to
provide only "raw capabilities". There exist many tools and systems that
provide the raw capabilities. The challenge is to provide these capabilities
along with considerations of usability and extensibility.

For many reasons, the traditional approach for building workflow systems has
been to build as much of the required capability as possible into the system
itself, relying very little on external services or even third party code.
This does not track well with the course of history since high-level
functionality tends to slowly creep down the software stack and into kernels,
kernel services, and system libraries.

Historically, workflow systems were developed to support “big science”
projects such as the LHC Experiments, LIGO Project and other community-wide
projects. Big Science projects were (and are still) characterized by the
availability of "workflow specialists" available to support the  development
and deployment of community workflow systems. Further, community-wide projects
were characterized by a select few important workflows, which were the primary
recipients of specialists' efforts. Once developed, a workflow system would
be used by hundreds of researchers over multiple years.

However, workflows aren’t what they used to be, for multiple reasons. First
workflows are often the representation of methodological advances and are thus
more pervasive, short-lived and wide-ranging. Further, they are no longer
confined to “big science” projects and sophisticated workflows are needed by
multiple science projects, which leads to diverse “design points” and thus
making it unlikely that “one size fits all”.  The ability to prototype, test
and experiment with workflows at scale suggests a need for interfaces and
middleware services that enable the rapid development of resources.

The proliferation of workflows and uptake by a wider range of science projects
(long-tail) implies that there is a need to support both end-users of worklows,
as well as workflow system and tool developers!

Jha and Turilli discuss this trend as it relates to workflows from a 
cyber-infrastructure perspective and to existing large-scale scientific workflow
efforts, \cite{jha_building_2016}. They propose that, while historically
successful, monolithic workflow systems present many problems for users,
developers, and maintainers. Instead, they propose that a new ``Lego style''
approach might work better where individual ``building blocks'' of capability
are assembled into the final workflow product. These building blocks would
include things like programming interfaces to queuing systems, programmable
pilot systems for scheduling jobs, workload balancers, and ensemble execution
tools, among others.

This approach would greatly improve both interoperability and sustainability
because it would standardize the programming interfaces and backends used by
workflow management systems. Jha et al. are developing a white paper to address
this further, \cite{jha_towards_2016}.

