\section{Background on Workflow Models}\label{workflows}

As mentioned previously, one of the most challenging aspects of studying
workflows is the way the vocabulary has been overloaded unintentionally.
It is somewhat clearer to understand them by considering the historial
perspective.

The use and study of workflows and the initial implementation of
workflow management systems developed in the business world with the
need to automate business processes. Lud\"{a}scher et al. ascribe the
origins of workflows and workflow management systems to ``office
automation'' trends in the 1970s, \cite{ludascher_scientific_2006}. Van Der
Aalst argues that ``workflows'' arose from the needs of businesses to not only
execute tasks, but ``to manage the flow of work through the
organization,'' and that managing workflows is the natural evolution
from the monolithic applications of the 1960s to applications that rely
on external functionality in the 1990s, \cite{van_der_aalst_application_1998}.
\footnote{One might argue that Van Der Aalst's depiction continues today with the
growth of the ``microservices'' architectural movement.} By 1995, in the
presence of many workflow tools, the Workflow Management Coalition had developed a ``standard'' definition of
workflows, \cite{hollingsworth_workflow_1993},

\begin{displayquote}
A Workflow is the automation of a business process, in whole or part, during
which documents, information or tasks are passed from one participant (a 
resource; human or machine) to another for action, according a set of 
procedural rules. 
\end{displayquote}

In the early 2000s, workflow systems started finding use in scientific
contexts where process automation was required for scientific uses
instead of traditional business uses. The focus of scientific workflows,
at the time, also shifted to focus primarily on data processing for
large ``grids'' of networked services, \cite{yu_taxonomy_2005}. Yu and Buyya
define a workflow as

\begin{displayquote}
... a collection of tasks that are processed on distributed resources in a
well-defined order to accomplish a specific goal.
\end{displayquote}

This latter definition is important because of what is missing: the human
element. For many in the grid/eScience workflows community this has become
the standard definition of a workflow and the involvement of humans
results not in a single workflow, but multiple workflows spanned by a human.
Machines or instruments are absent from the definition as well, but in
practice many modern grid workflows are launched automatically
when data ``comes off'' of instruments because they remain the primary
source of data in grid workflows, (c.f. - \cite{megino_panda:_2015}).

In addition to ``grid workflows,'' the scientific community started
exploring ``modeling and simulation workflows'' which focus not on data
flow, but on the orchestration of activities related to modeling and
simulation instead, sometimes on small local computers, but often on the
largest of the world's ``Leadership Class'' supercomputers. Unlike grid
workflows they tend to require human interaction in one way or another.
Some of these workflows are defined in the context of a particular way
of working, such as the Automation, Data, Environment, and Sharing
(ADES) model of Pizzi et al., \cite{pizzi_aiida:_2016}, the
``Design-to-Analysis'' model of Clay et al., \cite{clay_incorporating_2015}, or
the model of Billings et al.,\cite{billings_eclipse_2017}. Many scientific workflows, while exceptionally well defined and capable, remain hard-coded into dedicated environments - 
not general purpose workflow systems - developed for the sole purpose of
executing that single or at most a few related workflows, \cite{lingerfelt_beam:_2016}.

Additional types of workflows in the scientific community include
workflows that process ensembles of calculations for uncertainty quantification, verification and validation or probabilistic risk assessment, \cite{montoya_apex_2016},
and workflows that are used for testing software. These workflows share the property that they are all running a very large collection of jobs that only provide value when run together. However, they differ because testing workflows typically run each test as an independent task, whereas the other workflows may or may not change the tasks that are executed based on the intermediate state of the entire ensemble. These workflows require a large cluster or possibly a supercomputer in extreme cases.

The above demonstrates the diversity in the workflow science space and that the very understanding of what a workflow is tends to vary between communities. However, this should not be taken to mean there is no common functionality between workflow management systems in different spaces. Many sources in the literature indicate that the contrary is in fact true: there is significant duplication and commonality in this space. The overlap in these technologies is typically demonstrated by developing taxonomies or classification systems.

\subsection{Taxonomies and
Classification}\label{taxonomies-and-classification}
\todo{Why are we talking about taxonomies and classification?}
There have been several efforts to classify, survey or develop taxonomies
for workflows. Yu and Buyya present an exceptional and noteworthy taxonomy for grid
workflows. Multiple other efforts provide highly useful vocabularies and analyses as
well.

Yu and Buyya developed a taxonomy for workflow management systems on
grids that sought to capture the architectural style while identifying
comparison criteria, \cite{yu_taxonomy_2005}. Their work is notable because it
largely avoids a discussion of applications and focuses purely on the
functional properties of the workflow management systems as they exist
on the grids. Yu and Buyya root their taxonomy on five core elements of
grid-based workflow management systems: workflow design, information
retrieval, workflow scheduling, fault tolerance, and data movement.
While many of the properties and taxonomic elements they describe seem
common to all systems, others would appear to be grid-specific at present, such
as ``Workflow QoS Constraints''. Their work also shows how thirteen common grid
workflow management systems, including Pegasus and Kepler, are covered by the
taxonomy. Like other authors, Yu and Buyya cite the lack of standardized
workflow syntax and language as sources of interoperability issues. Their work is extremely
detailed and a very helpful resource for understanding grid workflows.

As a counter point to the often automated grid workflows, human involvement is 
critical in cases which require adaptive
management, as shown by Han and Bussler, \cite{han_taxonomy_1998}. Their work
considers adaptive workflow management in the context of healthcare
workflows and argues that workflow technology in 2002 was incapable of
adapting sufficiently to meet the unexpected needs of medical personnel.
Along with unexpected needs (``changing environment''), they cite the
evolution of software systems (``technical advances'') as another
critical area that leads to required changes in workflow management
systems.

Han and Bussler also discuss situations that lead to ``ad-hoc derivation'' of
workflows. ``Ad-hoc derivation'' in this case means generating extra
workflow steps or details, such as converting from an abstract to a
concrete workflow as Pegasus and other grid workflow systems do.
Specifically, Han and Bussler cite dynamic refinement, user involvement,
unpredictable events and erroneous situations as systems that require
the workflow to behave in an unplanned way, and for which
workflow managements systems should be prepared. Meta-models, open-point
(more commonly known as ``extension point'') or hybrid approaches are
proposed as solutions.

It is important to note that Han and Bussler only consider business
workflows and management systems, not scientific workflows and
management systems. In this context they also share some important
wisdom that is, arguably, of equal importance to scientific workflows:

\begin{displayquote}
Workflow systems do not exist for their own purposes. They
are a constituent component of a business system. A business system is usually domain
specific.
\end{displayquote}

This is an important consideration for scientific workflows because the
``business logic'' of scientific workflows is ``domain logic'' that is
highly specific to the scientific domain under consideration. This
necessarily leads to a diverse workflow science ecosystem.

Scientific workflow management systems have flourished since their
inception, although not without significant overlap and duplication of
effort. The survey of scientific workflow management systems by Barker
and Hemert illustrates both growth and growing pains while also providing
important observations and recommendations on the topic,
\cite{barker_scientific_2007}.

Barker and Hemert also provide key insights into the history of
workflow management systems as an important part of business
automation. The authors make an important comparison between traditional
business workflow management systems and their scientific counterparts,
citing in particular that traditional business workflow tools employ the
wrong abstraction for scientists They define workflows using the
``standard'' definition from the Workflow Management Coalition, (c.f. \S \ref{workflows} above).

The discussion points that Barker and Hemert raise are important because
of their continuing importance and relevance today, particularly the
need to enable programmability through standard languages instead of
custom, proprietary languages. Sticking to
standards is also important and perhaps illustrated best by Barker's and
Hemert's statement that

\begin{displayquote}
If software development and tool support terminates on one proprietary 
framework, workflows will need to be re-implemented from scratch.
\end{displayquote}

This is an important point even for workflow tools that do not use
proprietary standards, but ``roll their own'' solutions. What can be
done to support those tools and reproduce those workflows once support
for continued development ends?

Notably, in their discussion about the Kepler workflow management
system, Barker and Hemert state that

\begin{displayquote}
Actors are re-usable independent blocks of computation, such as:
Web Services, database calls, etc.
\end{displayquote}

This is consistent with the solution proposed in this paper and discussed in \S \ref{}.\todo{fix reference!}

Montoya et al. discuss workflow needs for the Alliance for Application
Performance at Extreme Scale (APEX), \cite{nersc_apex_2016}, and describe
three main classes of workflows: Simulation Science, Uncertainty
Quantification (UQ), and High Throughput Computing (HTC),
\cite{montoya_apex_2016}.
HTC workflows start with the collection of data from experiments that is in turn
transported to large compute facilities for
processing. Many grid workflows are HTC workflows, but not all HTC
workflows are grid workflows since some HTC workflows - such as those
those presented by Montoya et al. - may be run on large resources that
are not traditionally ``grid machines.'' When Montoya et al. describe scientific workflows,
they are refering to the modeling and simulation workflows described above. Montoya et al. also provide a
detailed mapping of each workflow type to optimal hardware resources for the APEX
program.

The U.S. Department of Energy sponsored the \emph{DOE NGNS/CS Scientific
Workflows Workshop} on April 20-21st 2015. In the report, Deelman et al.
describe the requirements and research directions for scientific
workflows for the exascale environment, \cite{deelman_future_2015}\cite{deelman_future_2017}. The report (and paper) describes scientific workflows primarily by three application types:
Simulations, Instruments, and Collaborations. The findings of the workshop are
comprehensive and encouraging, with recommendations for research
priorities in Application Requirements, Hardware Systems, System
Software, Workflow Management System Design and Execution, Programming and Usability,
Provenance Capture, Validation, and Workflow Science.

The definitions of a ``workflow'' and ``workflow management systems''
are thoroughly explored and put into context for the purposes of the
workshop. The authors of the report are very careful to define workflows
not just as a collection of managed processes, which is common, but in
such a way that it is clear that reproducibility, mobility and some
degree of generality are required by both the description of the
workflow and the management system. \footnote{The report appears to provide
three separate definitions for ``workflow'' on pages 6, 9 and 10.}

Ferreira da Silva et al. attempt to characterize workflow management systems in 
\cite{ferreira_da_silva_characterization_nodate}. The authors reduce key properties of workflow
systems into four incongruent areas: (i) design, (ii) execution and monitoring, (iii)
reusability and (iv) collaboration. These properties are essential
considerations for most  software with limited specificity for workflow
management systems. Furthermore, there is general conflation between
classification and taxonomy and significant incoherence between entries in
equivalence classes. Most significantly, it fluctuates somewhat chaotically
between discussing workflows and workflow management systems without linking
workflow properties to successful design and properties of workflow systems.

\subsection{Interoperability}

Uncertainty in the very definition of workflows and workflow management systems makes it
difficult or impossible to address complicated workflows that may require interoperability
across computing facilities or distribution of ``sub-workflows'' across multiple systems.
For example, Session IV of the Twentieth Anniversary Meeting of the SOS Workshop
(SOS20) focused on workflow and workflow management
system development activities of the three participating institutions: Sandia
National Laboratory, Oak Ridge National Laboratory, and the Swiss National Supercomputing
Centre, \cite{pack_sos20_2016}. Multiple
presenters illustrated the challenges facing the workflow science community and
widely agreed that no single workflow management system could satisfy
all the needs of those present. Instead, attendees proposed that the community as a whole would
be served best by seeking to enable interoperability where possible. 

Similar considerations are present in reports and discussions surrounding the future of
workflow management in the Leadership Computing Facilities where the ``proliferation'' of
workflow management systems and the lack of a consistent definition of a workflow are 
signficant barriers to the adoption of this technology. The High Performance Computing Facility
Operational Assessment 2015: Oak Ridge Leadership Computing Facility (OLCF)
report, \cite{barker_scientific_2007}, illustrates the problem that such facilities
face,

\begin{displayquote}
These discussions concluded with the observation that the current proliferation
of workflow systems in response to perceived domain-specific needs of 
scientific workflows makes it difficult to choose a site-wide operational
workflow manager, particularly for the leadership-class machines. However,
there are opportunities where facilities can centralize workflow technology 
offerings to reduce anticipated fragmentation. This is especially true if a 
facility attempts to develop, deploy, and operate each and every workflow 
solution requested by the user community. Through these evaluations, the OLCF
seeks to identify interesting intersections that are of the most value to OLCF
stakeholders.
\end{displayquote}

The Oak Ridge Leadership Computing Facility (OLCF) is not alone in this
because other Leadership Computing Facilities face the same problem. Few
of these facilities have discussed the possibility every facility may end up
supporting different workflows systems entirely such that workflows at
one facility can not be run at another without significant work to
install one or more additional workflow management systems!

This idea is also illustrated well in The Future of Scientific Workflows report by Deelman et  
al. through the concept of the ``large-scale science campaign,'' \cite{deelman_future_2015}.
Such a campaign integrates multiple workflows, not necessarily all in the same workflow
management system, to perform data acquisition from experimental equipment, modeling and analysis with supercomputers, and data analysis with either grids computing or
supercomputers.\footnote{This is, in essence, the same problem that
PanDA/BigPanDA solves for the Large Hadron Collider. So, while it is not
a new problem, it is increasingly common.} An interesting case study is the ICEMAN project at Oak Ridge National Laboratory, (on which Mr. Billings is a co-principal investigator). ICEMAN
seeks to use a combination of instruments from the Spallation Neutron
Source, OLCF compute resources such as the Titan supercomputer, the
Laboratory's Compute Advanced Data Environment for Science (CADES), the
Eclipse Integrated Computational Environment for modeling and simulation workflows, and AiiDA (c.f. - \cite{pizzi_aiida:_2016}) to automate the analysis of inelastic and quasielastic neutron scattering data as part of a comprehensive problem solving environment for scientists.

The brief summary of different workflow models and related problems above illustrate the
diversity in the ``marketplace'' and illustrates the lack of a coherent understanding of 
workflows or search for higher level concepts. At this point it should be clear that there are 
two significant problems facing the workflow science community:
\begin{itemize}
  \item Everyone has their own workflow tools, and no two tool chains are the same.
  \item There is very little interoperability in the workflow tool space.
\end{itemize} The following sections propose that enough work has
been done experimentally and that the community would greatly benefit from seeking a
higher level of understanding and standardization through common building blocks.
